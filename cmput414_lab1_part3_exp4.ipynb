{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "81f1b20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from dataIO import evaluation_entry\n",
    "\n",
    "\n",
    "from dataIO import loadImgs_plus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f7df77f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BGNet(nn.Module):\n",
    "    \n",
    "    # get 1000 1 6 1 shape\n",
    "    # predict forground or background\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # set stride , kernel size to 1 at the begining, and tune each layers\n",
    "        \n",
    "        # Conv layer 1\n",
    "        self.conv1 = nn.Conv2d(1,  16, (3, 1), stride = 1, padding = (1, 0))\n",
    "        self.b_norm1 = nn.BatchNorm2d(16)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(16, 32, (3, 1), stride = 1, padding = (0, 0))\n",
    "        self.b_norm2 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(32, 64, (1, 1), stride = 1, padding = (0, 0))\n",
    "        self.b_norm3 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(64, 128, (1, 1), stride = 1, padding = (0, 0))\n",
    "        self.b_norm4 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.conv5 = nn.Conv2d(128, 256, (1, 1), stride = 2, padding = (0, 0))\n",
    "        self.b_norm5 = nn.BatchNorm2d(256)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "        \n",
    "        # TODO\n",
    "        self.fc1 = nn.Linear(512,128)\n",
    "        self.fc2 = nn.Linear(128,64)\n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        self.fc4 = nn.Linear(32, 16)\n",
    "        self.fc5 = nn.Linear(16, 2)\n",
    "\n",
    "\n",
    "    def forward(self, inputdata):\n",
    "        \n",
    "        # 1000 1 6 1\n",
    "        #print(\"Inputdata shape: \",inputdata.shape)\n",
    "        \n",
    "        x = inputdata.unsqueeze(-1).unsqueeze(1)\n",
    "        x = self.conv1(x)\n",
    "        x = self.b_norm1(x)\n",
    "        x = F.relu(x)\n",
    "        #x = F.max_pool2d(x, 2, 2)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.b_norm2(x)\n",
    "        x = F.relu(x)\n",
    "        #x = F.max_pool2d(x, 2, 2)\n",
    "        #print(x.shape)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.b_norm3(x)\n",
    "        x = F.relu(x)\n",
    "        #x = F.max_pool2d(x, 2, 2)\n",
    "        \n",
    "        x = self.conv4(x)\n",
    "        x = self.b_norm4(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.conv5(x)\n",
    "        #print(\"conv5\", x.shape)\n",
    "        \n",
    "        x = self.b_norm5(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = x.view(-1, 512) # 扁平化flat然后传入全连接层 ex [1000,256, 2, 1] => 256*2*1\n",
    "\n",
    "        x = F.relu(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.fc4(x)\n",
    "        \n",
    "        #random dropout\n",
    "        #x = self.dropout(x)\n",
    "        \n",
    "        x = self.fc5(x)\n",
    "        \n",
    "        \n",
    "        return F.log_softmax(x, dim = 1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b5543831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BGNet(\n",
      "  (conv1): Conv2d(1, 16, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0))\n",
      "  (b_norm1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv2): Conv2d(16, 32, kernel_size=(3, 1), stride=(1, 1))\n",
      "  (b_norm2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv3): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (b_norm3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv4): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (b_norm4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv5): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2))\n",
      "  (b_norm5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dropout): Dropout(p=0.25, inplace=False)\n",
      "  (fc1): Linear(in_features=512, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (fc4): Linear(in_features=32, out_features=16, bias=True)\n",
      "  (fc5): Linear(in_features=16, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# my test\n",
    "# TODO\n",
    "input_test = torch.rand((1000, 6))\n",
    "net = BGNet()\n",
    "net.forward(input_test)\n",
    "print(net)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eb57b9f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading file: in001189.jpg\n",
      "loading file: gt001189.png gt000399.png\n",
      "epoch: 0  loss: 65.35711920261383\n",
      "epoch: 1  loss: 54.98476767539978\n",
      "epoch: 2  loss: 45.75557625293732\n",
      "epoch: 3  loss: 37.33762666583061\n",
      "epoch: 4  loss: 30.20814609527588\n",
      "epoch: 5  loss: 24.958520099520683\n",
      "epoch: 6  loss: 21.501734130084515\n",
      "epoch: 7  loss: 19.231808550655842\n",
      "epoch: 8  loss: 17.592382051050663\n",
      "epoch: 9  loss: 16.295651633292437\n",
      "epoch: 10  loss: 15.209456983953714\n",
      "epoch: 11  loss: 14.264872938394547\n",
      "epoch: 12  loss: 13.42459074780345\n",
      "epoch: 13  loss: 12.669933069497347\n",
      "epoch: 14  loss: 11.99491212144494\n",
      "epoch: 15  loss: 11.394618524238467\n",
      "epoch: 16  loss: 10.863113267347217\n",
      "epoch: 17  loss: 10.393157113343477\n",
      "epoch: 18  loss: 9.977503450587392\n",
      "epoch: 19  loss: 9.60749259032309\n",
      "epoch: 20  loss: 9.27561592310667\n",
      "epoch: 21  loss: 8.976777391508222\n",
      "epoch: 22  loss: 8.705840004608035\n",
      "epoch: 23  loss: 8.459116897545755\n",
      "epoch: 24  loss: 8.23216029908508\n",
      "epoch: 25  loss: 8.022053072229028\n",
      "epoch: 26  loss: 7.826190574094653\n",
      "epoch: 27  loss: 7.641361588612199\n",
      "epoch: 28  loss: 7.466495435684919\n",
      "epoch: 29  loss: 7.300791714340448\n",
      "epoch: 30  loss: 7.143326788209379\n",
      "epoch: 31  loss: 6.993470442481339\n",
      "epoch: 32  loss: 6.8511582827195525\n",
      "epoch: 33  loss: 6.716178367845714\n",
      "epoch: 34  loss: 6.587860974483192\n",
      "epoch: 35  loss: 6.467057368252426\n",
      "epoch: 36  loss: 6.353034109342843\n",
      "epoch: 37  loss: 6.245108112692833\n",
      "epoch: 38  loss: 6.1424787025898695\n",
      "epoch: 39  loss: 6.04590715887025\n",
      "epoch: 40  loss: 5.954043718520552\n",
      "epoch: 41  loss: 5.867141712456942\n",
      "epoch: 42  loss: 5.7843688181601465\n",
      "epoch: 43  loss: 5.7058597835712135\n",
      "epoch: 44  loss: 5.631291845813394\n",
      "epoch: 45  loss: 5.560653100721538\n",
      "epoch: 46  loss: 5.49290680536069\n",
      "epoch: 47  loss: 5.4286891664378345\n",
      "epoch: 48  loss: 5.367629644460976\n",
      "epoch: 49  loss: 5.3094604052603245\n",
      "epoch: 50  loss: 5.253972621867433\n",
      "epoch: 51  loss: 5.201485056895763\n",
      "epoch: 52  loss: 5.150692656869069\n",
      "epoch: 53  loss: 5.10233069001697\n",
      "epoch: 54  loss: 5.056481892475858\n",
      "epoch: 55  loss: 5.012838507071137\n",
      "epoch: 56  loss: 4.971185416448861\n",
      "epoch: 57  loss: 4.931180341169238\n",
      "epoch: 58  loss: 4.8929710001684725\n",
      "epoch: 59  loss: 4.856494341511279\n",
      "epoch: 60  loss: 4.821390406345017\n",
      "epoch: 61  loss: 4.787474872660823\n",
      "epoch: 62  loss: 4.755659673712216\n",
      "epoch: 63  loss: 4.724499195232056\n",
      "epoch: 64  loss: 4.695254806429148\n",
      "epoch: 65  loss: 4.666615893132985\n",
      "epoch: 66  loss: 4.638946174876764\n",
      "epoch: 67  loss: 4.612363023567013\n",
      "epoch: 68  loss: 4.586713324766606\n",
      "epoch: 69  loss: 4.562042181380093\n",
      "epoch: 70  loss: 4.537852138862945\n",
      "epoch: 71  loss: 4.514595965971239\n",
      "epoch: 72  loss: 4.492086484329775\n",
      "epoch: 73  loss: 4.470181431155652\n",
      "epoch: 74  loss: 4.448641118244268\n",
      "epoch: 75  loss: 4.427901033428498\n",
      "epoch: 76  loss: 4.40780324570369\n",
      "epoch: 77  loss: 4.387965104659088\n",
      "epoch: 78  loss: 4.36873843416106\n",
      "epoch: 79  loss: 4.349798574461602\n",
      "epoch: 80  loss: 4.331402526178863\n",
      "epoch: 81  loss: 4.3134961483301595\n",
      "epoch: 82  loss: 4.2960407916107215\n",
      "epoch: 83  loss: 4.279022836126387\n",
      "epoch: 84  loss: 4.262628594180569\n",
      "epoch: 85  loss: 4.246336231008172\n",
      "epoch: 86  loss: 4.230728714435827\n",
      "epoch: 87  loss: 4.215479506878182\n",
      "epoch: 88  loss: 4.200323584722355\n",
      "epoch: 89  loss: 4.185545213054866\n",
      "epoch: 90  loss: 4.171115595730953\n",
      "epoch: 91  loss: 4.156623420829419\n",
      "epoch: 92  loss: 4.142467639350798\n",
      "epoch: 93  loss: 4.128545892541297\n",
      "epoch: 94  loss: 4.11486456473358\n",
      "epoch: 95  loss: 4.101313689025119\n",
      "epoch: 96  loss: 4.0880384962656535\n",
      "epoch: 97  loss: 4.074760733812582\n",
      "epoch: 98  loss: 4.061835111584514\n",
      "epoch: 99  loss: 4.049217229534406\n",
      "epoch: 100  loss: 4.036519365734421\n",
      "epoch: 101  loss: 4.024292558955494\n",
      "epoch: 102  loss: 4.012169010180514\n",
      "epoch: 103  loss: 4.000307260314003\n",
      "epoch: 104  loss: 3.988662575779017\n",
      "epoch: 105  loss: 3.9772120974957943\n",
      "epoch: 106  loss: 3.9660863079479896\n",
      "epoch: 107  loss: 3.954996011161711\n",
      "epoch: 108  loss: 3.9441240339074284\n",
      "epoch: 109  loss: 3.933355702087283\n",
      "epoch: 110  loss: 3.9228232640307397\n",
      "epoch: 111  loss: 3.9125023783999495\n",
      "epoch: 112  loss: 3.902233818138484\n",
      "epoch: 113  loss: 3.8920247845817357\n",
      "epoch: 114  loss: 3.882044964208035\n",
      "epoch: 115  loss: 3.871948073297972\n",
      "epoch: 116  loss: 3.862117538781604\n",
      "epoch: 117  loss: 3.8525395887263585\n",
      "epoch: 118  loss: 3.842992133781081\n",
      "epoch: 119  loss: 3.833576278120745\n",
      "epoch: 120  loss: 3.824344465334434\n",
      "epoch: 121  loss: 3.8151509882882237\n",
      "epoch: 122  loss: 3.806156714592362\n",
      "epoch: 123  loss: 3.7973246661713347\n",
      "epoch: 124  loss: 3.788632067007711\n",
      "epoch: 125  loss: 3.7799541587301064\n",
      "epoch: 126  loss: 3.7714713783643674\n",
      "epoch: 127  loss: 3.763205015537096\n",
      "epoch: 128  loss: 3.754836034728214\n",
      "epoch: 129  loss: 3.7465899045346305\n",
      "epoch: 130  loss: 3.738500940031372\n",
      "epoch: 131  loss: 3.7305219952831976\n",
      "epoch: 132  loss: 3.7226569801277947\n",
      "epoch: 133  loss: 3.71492970304098\n",
      "epoch: 134  loss: 3.707451590045821\n",
      "epoch: 135  loss: 3.6998735173838213\n",
      "epoch: 136  loss: 3.6924885475891642\n",
      "epoch: 137  loss: 3.685158091568155\n",
      "epoch: 138  loss: 3.677858801704133\n",
      "epoch: 139  loss: 3.6703626054513734\n",
      "epoch: 140  loss: 3.663060900667915\n",
      "epoch: 141  loss: 3.655794234829955\n",
      "epoch: 142  loss: 3.6485832232865505\n",
      "epoch: 143  loss: 3.6415439139236696\n",
      "epoch: 144  loss: 3.634759114822373\n",
      "epoch: 145  loss: 3.6280000315164216\n",
      "epoch: 146  loss: 3.621368768013781\n",
      "epoch: 147  loss: 3.614734012982808\n",
      "epoch: 148  loss: 3.6081020277633797\n",
      "epoch: 149  loss: 3.6015536310733296\n",
      "epoch: 150  loss: 3.5951124368875753\n",
      "epoch: 151  loss: 3.588683729438344\n",
      "epoch: 152  loss: 3.582122340827482\n",
      "epoch: 153  loss: 3.5759291987342294\n",
      "epoch: 154  loss: 3.569725681823911\n",
      "epoch: 155  loss: 3.5636416250199545\n",
      "epoch: 156  loss: 3.5575172719254624\n",
      "epoch: 157  loss: 3.5515453164407518\n",
      "epoch: 158  loss: 3.545563600229798\n",
      "epoch: 159  loss: 3.5395757262595\n",
      "epoch: 160  loss: 3.5335842993226834\n",
      "epoch: 161  loss: 3.5277138920209836\n",
      "epoch: 162  loss: 3.521955781747238\n",
      "epoch: 163  loss: 3.5161365889944136\n",
      "epoch: 164  loss: 3.510397055550129\n",
      "epoch: 165  loss: 3.5045964416494826\n",
      "epoch: 166  loss: 3.499071354555781\n",
      "epoch: 167  loss: 3.493383284047013\n",
      "epoch: 168  loss: 3.487778473921935\n",
      "epoch: 169  loss: 3.4823452111158986\n",
      "epoch: 170  loss: 3.4769469904567814\n",
      "epoch: 171  loss: 3.4715553618298145\n",
      "epoch: 172  loss: 3.4661172487249132\n",
      "epoch: 173  loss: 3.4608797771215905\n",
      "epoch: 174  loss: 3.4556777149409754\n",
      "epoch: 175  loss: 3.450494495438761\n",
      "epoch: 176  loss: 3.4454142496542772\n",
      "epoch: 177  loss: 3.4403160971560283\n",
      "epoch: 178  loss: 3.43525705493812\n",
      "epoch: 179  loss: 3.4301363056292757\n",
      "epoch: 180  loss: 3.4253144658578094\n",
      "epoch: 181  loss: 3.4203891353390645\n",
      "epoch: 182  loss: 3.415549952231231\n",
      "epoch: 183  loss: 3.410661221918417\n",
      "epoch: 184  loss: 3.4059100969461724\n",
      "epoch: 185  loss: 3.401086633297382\n",
      "epoch: 186  loss: 3.3962803179601906\n",
      "epoch: 187  loss: 3.39149511566211\n",
      "epoch: 188  loss: 3.3868490489112446\n",
      "epoch: 189  loss: 3.381941085885046\n",
      "epoch: 190  loss: 3.377349076210521\n",
      "epoch: 191  loss: 3.3726532203436363\n",
      "epoch: 192  loss: 3.3680033694108715\n",
      "epoch: 193  loss: 3.3634814686956815\n",
      "epoch: 194  loss: 3.358799041772727\n",
      "epoch: 195  loss: 3.3543594622897217\n",
      "epoch: 196  loss: 3.3497165363223758\n",
      "epoch: 197  loss: 3.3453792379877996\n",
      "epoch: 198  loss: 3.3409221242327476\n",
      "epoch: 199  loss: 3.3365452496072976\n",
      "epoch: 200  loss: 3.3322274405218195\n",
      "epoch: 201  loss: 3.3279325199546292\n",
      "epoch: 202  loss: 3.323549337539589\n",
      "epoch: 203  loss: 3.319144887747825\n",
      "epoch: 204  loss: 3.314913469585008\n",
      "epoch: 205  loss: 3.310567982509383\n",
      "epoch: 206  loss: 3.3063414613425266\n",
      "epoch: 207  loss: 3.302204823063221\n",
      "epoch: 208  loss: 3.298007390505518\n",
      "epoch: 209  loss: 3.293928423736361\n",
      "epoch: 210  loss: 3.2896800258458825\n",
      "epoch: 211  loss: 3.285677478343132\n",
      "epoch: 212  loss: 3.2816305949090747\n",
      "epoch: 213  loss: 3.27753860213852\n",
      "epoch: 214  loss: 3.273581903609738\n",
      "epoch: 215  loss: 3.269615304932813\n",
      "epoch: 216  loss: 3.265626550935849\n",
      "epoch: 217  loss: 3.261820398569398\n",
      "epoch: 218  loss: 3.2578027638664935\n",
      "epoch: 219  loss: 3.2540145829698304\n",
      "epoch: 220  loss: 3.250149816973135\n",
      "epoch: 221  loss: 3.246349766966887\n",
      "epoch: 222  loss: 3.242515037294652\n",
      "epoch: 223  loss: 3.238684916344937\n",
      "epoch: 224  loss: 3.235058149242832\n",
      "epoch: 225  loss: 3.231196913511667\n",
      "epoch: 226  loss: 3.227413337051985\n",
      "epoch: 227  loss: 3.2237419498560484\n",
      "epoch: 228  loss: 3.219938287496916\n",
      "epoch: 229  loss: 3.2163338929894962\n",
      "epoch: 230  loss: 3.212597027835727\n",
      "epoch: 231  loss: 3.2089455666500726\n",
      "epoch: 232  loss: 3.2053438100483618\n",
      "epoch: 233  loss: 3.201738566349377\n",
      "epoch: 234  loss: 3.1981864924673573\n",
      "epoch: 235  loss: 3.1945519125147257\n",
      "epoch: 236  loss: 3.191022860868543\n",
      "epoch: 237  loss: 3.1873461336872424\n",
      "epoch: 238  loss: 3.1838941516907653\n",
      "epoch: 239  loss: 3.1802391936435015\n",
      "epoch: 240  loss: 3.176669752894668\n",
      "epoch: 241  loss: 3.173246514517814\n",
      "epoch: 242  loss: 3.169612379540922\n",
      "epoch: 243  loss: 3.1663978200303973\n",
      "epoch: 244  loss: 3.162776926001243\n",
      "epoch: 245  loss: 3.1594960442962474\n",
      "epoch: 246  loss: 3.155980956791609\n",
      "epoch: 247  loss: 3.1527571224214626\n",
      "epoch: 248  loss: 3.1493358637962956\n",
      "epoch: 249  loss: 3.145905183548166\n",
      "epoch: 250  loss: 3.142549174001033\n",
      "epoch: 251  loss: 3.1392363698178087\n",
      "epoch: 252  loss: 3.135791707674798\n",
      "epoch: 253  loss: 3.13240628708445\n",
      "epoch: 254  loss: 3.1289643268901273\n",
      "epoch: 255  loss: 3.1256286839488894\n",
      "epoch: 256  loss: 3.1222891877914662\n",
      "epoch: 257  loss: 3.1189036815267173\n",
      "epoch: 258  loss: 3.115747898882546\n",
      "epoch: 259  loss: 3.112318288978713\n",
      "epoch: 260  loss: 3.1091134369489737\n",
      "epoch: 261  loss: 3.1058901279466227\n",
      "epoch: 262  loss: 3.1026821811828995\n",
      "epoch: 263  loss: 3.09948844214523\n",
      "epoch: 264  loss: 3.0962791522906628\n",
      "epoch: 265  loss: 3.0930565953749465\n",
      "epoch: 266  loss: 3.0898817879351554\n",
      "epoch: 267  loss: 3.0864325424772687\n",
      "epoch: 268  loss: 3.0833859405101975\n",
      "epoch: 269  loss: 3.0801263344546896\n",
      "epoch: 270  loss: 3.076855667961354\n",
      "epoch: 271  loss: 3.073751629322942\n",
      "epoch: 272  loss: 3.070443641860038\n",
      "epoch: 273  loss: 3.0672200885019265\n",
      "epoch: 274  loss: 3.0639735024233232\n",
      "epoch: 275  loss: 3.0608315310892067\n",
      "epoch: 276  loss: 3.0575592026943923\n",
      "epoch: 277  loss: 3.0544227423888515\n",
      "epoch: 278  loss: 3.0511993001418887\n",
      "epoch: 279  loss: 3.0480234431634017\n",
      "epoch: 280  loss: 3.0448378884320846\n",
      "epoch: 281  loss: 3.041690754111187\n",
      "epoch: 282  loss: 3.0385527977086895\n",
      "epoch: 283  loss: 3.0355382107118203\n",
      "epoch: 284  loss: 3.032385387690738\n",
      "epoch: 285  loss: 3.029380260293692\n",
      "epoch: 286  loss: 3.026242326846841\n",
      "epoch: 287  loss: 3.023171339489636\n",
      "epoch: 288  loss: 3.020082307029952\n",
      "epoch: 289  loss: 3.017028014171956\n",
      "epoch: 290  loss: 3.0140601661878463\n",
      "epoch: 291  loss: 3.010991087252478\n",
      "epoch: 292  loss: 3.008116433094983\n",
      "epoch: 293  loss: 3.0049914197625185\n",
      "epoch: 294  loss: 3.0020265136336093\n",
      "epoch: 295  loss: 2.999117871182534\n",
      "epoch: 296  loss: 2.996051937541779\n",
      "epoch: 297  loss: 2.9932359634658496\n",
      "epoch: 298  loss: 2.990239795151865\n",
      "epoch: 299  loss: 2.9873901087667036\n",
      "epoch: 300  loss: 2.9843935572098417\n",
      "epoch: 301  loss: 2.9815511675005837\n",
      "epoch: 302  loss: 2.9786596649792045\n",
      "epoch: 303  loss: 2.975810713654937\n",
      "epoch: 304  loss: 2.973029028304154\n",
      "epoch: 305  loss: 2.970209397659346\n",
      "epoch: 306  loss: 2.967428925290733\n",
      "epoch: 307  loss: 2.9647415401304897\n",
      "epoch: 308  loss: 2.961941696361464\n",
      "epoch: 309  loss: 2.959137434085278\n",
      "epoch: 310  loss: 2.9564699888323958\n",
      "epoch: 311  loss: 2.9536281143555243\n",
      "epoch: 312  loss: 2.9510111146519193\n",
      "epoch: 313  loss: 2.9481935688054364\n",
      "epoch: 314  loss: 2.945561062610068\n",
      "epoch: 315  loss: 2.9428204293653835\n",
      "epoch: 316  loss: 2.9401257659592375\n",
      "epoch: 317  loss: 2.937420473645034\n",
      "epoch: 318  loss: 2.9346798640763154\n",
      "epoch: 319  loss: 2.9321772573857743\n",
      "epoch: 320  loss: 2.929369806075556\n",
      "epoch: 321  loss: 2.926863535471057\n",
      "epoch: 322  loss: 2.9242783381487243\n",
      "epoch: 323  loss: 2.921572452862165\n",
      "epoch: 324  loss: 2.9190814170142403\n",
      "epoch: 325  loss: 2.9163911290306714\n",
      "epoch: 326  loss: 2.9138518706604373\n",
      "epoch: 327  loss: 2.911312207517767\n",
      "epoch: 328  loss: 2.9087309644237394\n",
      "epoch: 329  loss: 2.906234842554113\n",
      "epoch: 330  loss: 2.9037059321490233\n",
      "epoch: 331  loss: 2.9011482756868645\n",
      "epoch: 332  loss: 2.898511967599916\n",
      "epoch: 333  loss: 2.896150671174837\n",
      "epoch: 334  loss: 2.893561443175713\n",
      "epoch: 335  loss: 2.8910681881206983\n",
      "epoch: 336  loss: 2.888452199225867\n",
      "epoch: 337  loss: 2.8860003025511105\n",
      "epoch: 338  loss: 2.8834447529516183\n",
      "epoch: 339  loss: 2.8809174763919145\n",
      "epoch: 340  loss: 2.8784951542547788\n",
      "epoch: 341  loss: 2.8758587321353843\n",
      "epoch: 342  loss: 2.873456416218687\n",
      "epoch: 343  loss: 2.8711207127489615\n",
      "epoch: 344  loss: 2.8683332410764706\n",
      "epoch: 345  loss: 2.866128450838005\n",
      "epoch: 346  loss: 2.8633545474040147\n",
      "epoch: 347  loss: 2.861106398922857\n",
      "epoch: 348  loss: 2.85837159570292\n",
      "epoch: 349  loss: 2.856032418196264\n",
      "epoch: 350  loss: 2.853451673498057\n",
      "epoch: 351  loss: 2.8510477984709723\n",
      "epoch: 352  loss: 2.848569577496164\n",
      "epoch: 353  loss: 2.8459783308626356\n",
      "epoch: 354  loss: 2.8436122881394112\n",
      "epoch: 355  loss: 2.8410410963097092\n",
      "epoch: 356  loss: 2.838668099428105\n",
      "epoch: 357  loss: 2.8361928326030466\n",
      "epoch: 358  loss: 2.8337443371528934\n",
      "epoch: 359  loss: 2.831376486885347\n",
      "epoch: 360  loss: 2.8287916032768408\n",
      "epoch: 361  loss: 2.826502490124767\n",
      "epoch: 362  loss: 2.823941600152466\n",
      "epoch: 363  loss: 2.821591527050259\n",
      "epoch: 364  loss: 2.8191469537432567\n",
      "epoch: 365  loss: 2.8167560764250084\n",
      "epoch: 366  loss: 2.8142737875368766\n",
      "epoch: 367  loss: 2.8120164887841383\n",
      "epoch: 368  loss: 2.8096298881446273\n",
      "epoch: 369  loss: 2.8071090016092057\n",
      "epoch: 370  loss: 2.804856293325429\n",
      "epoch: 371  loss: 2.802477360910416\n",
      "epoch: 372  loss: 2.8001762634121405\n",
      "epoch: 373  loss: 2.7977478937373235\n",
      "epoch: 374  loss: 2.795578798821225\n",
      "epoch: 375  loss: 2.793063563049145\n",
      "epoch: 376  loss: 2.7907836206868524\n",
      "epoch: 377  loss: 2.788362843944924\n",
      "epoch: 378  loss: 2.786195232132741\n",
      "epoch: 379  loss: 2.7837904657772015\n",
      "epoch: 380  loss: 2.7816360100950988\n",
      "epoch: 381  loss: 2.779250160090669\n",
      "epoch: 382  loss: 2.7770789347505342\n",
      "epoch: 383  loss: 2.774542439959987\n",
      "epoch: 384  loss: 2.7727329594599723\n",
      "epoch: 385  loss: 2.7701574548300414\n",
      "epoch: 386  loss: 2.7678710960317403\n",
      "epoch: 387  loss: 2.76569695037324\n",
      "epoch: 388  loss: 2.7636030343783204\n",
      "epoch: 389  loss: 2.7609757185509807\n",
      "epoch: 390  loss: 2.7590381406971574\n",
      "epoch: 391  loss: 2.7567251132331876\n",
      "epoch: 392  loss: 2.754457855819055\n",
      "epoch: 393  loss: 2.7521389106623246\n",
      "epoch: 394  loss: 2.750105520557554\n",
      "epoch: 395  loss: 2.747786780952083\n",
      "epoch: 396  loss: 2.7455241511706845\n",
      "epoch: 397  loss: 2.7434203489847278\n",
      "epoch: 398  loss: 2.7412008935334597\n",
      "epoch: 399  loss: 2.7390800659704837\n",
      "epoch: 400  loss: 2.73678262525209\n",
      "epoch: 401  loss: 2.7346778038809134\n",
      "epoch: 402  loss: 2.732450588267966\n",
      "epoch: 403  loss: 2.7301343610324693\n",
      "epoch: 404  loss: 2.728066055393356\n",
      "epoch: 405  loss: 2.7258857322049153\n",
      "epoch: 406  loss: 2.7236908103950555\n",
      "epoch: 407  loss: 2.721415737947609\n",
      "epoch: 408  loss: 2.7192517317726015\n",
      "epoch: 409  loss: 2.7172029777993885\n",
      "epoch: 410  loss: 2.7147951426159125\n",
      "epoch: 411  loss: 2.7128250413079513\n",
      "epoch: 412  loss: 2.7105206016913144\n",
      "epoch: 413  loss: 2.7084693107099156\n",
      "epoch: 414  loss: 2.7062833612399118\n",
      "epoch: 415  loss: 2.704011529503987\n",
      "epoch: 416  loss: 2.7019640772541607\n",
      "epoch: 417  loss: 2.699816177273533\n",
      "epoch: 418  loss: 2.6976595946489397\n",
      "epoch: 419  loss: 2.6954260086549766\n",
      "epoch: 420  loss: 2.693365936534974\n",
      "epoch: 421  loss: 2.6911956225558242\n",
      "epoch: 422  loss: 2.689025180990029\n",
      "epoch: 423  loss: 2.6868980294175344\n",
      "epoch: 424  loss: 2.68466908983919\n",
      "epoch: 425  loss: 2.682545624778868\n",
      "epoch: 426  loss: 2.680425012055821\n",
      "epoch: 427  loss: 2.6782406970069133\n",
      "epoch: 428  loss: 2.6761916656514586\n",
      "epoch: 429  loss: 2.673901879004916\n",
      "epoch: 430  loss: 2.6717265801917165\n",
      "epoch: 431  loss: 2.6697567382698253\n",
      "epoch: 432  loss: 2.667557635295452\n",
      "epoch: 433  loss: 2.6653661437794653\n",
      "epoch: 434  loss: 2.6632212013773824\n",
      "epoch: 435  loss: 2.661343861162095\n",
      "epoch: 436  loss: 2.6589746576109974\n",
      "epoch: 437  loss: 2.656967183782399\n",
      "epoch: 438  loss: 2.654845483641111\n",
      "epoch: 439  loss: 2.652865558020494\n",
      "epoch: 440  loss: 2.6506170606971864\n",
      "epoch: 441  loss: 2.6486773693050054\n",
      "epoch: 442  loss: 2.6466906070609184\n",
      "epoch: 443  loss: 2.6445014219998484\n",
      "epoch: 444  loss: 2.6425947206807905\n",
      "epoch: 445  loss: 2.6403452486492824\n",
      "epoch: 446  loss: 2.6386720328537194\n",
      "epoch: 447  loss: 2.6362244511055906\n",
      "epoch: 448  loss: 2.634525114764074\n",
      "epoch: 449  loss: 2.6321821079845904\n",
      "epoch: 450  loss: 2.630412819827143\n",
      "epoch: 451  loss: 2.6281967625182006\n",
      "epoch: 452  loss: 2.6262407112662913\n",
      "epoch: 453  loss: 2.624225912165457\n",
      "epoch: 454  loss: 2.6222037288343927\n",
      "epoch: 455  loss: 2.6200246615480864\n",
      "epoch: 456  loss: 2.618113721339796\n",
      "epoch: 457  loss: 2.616090671056554\n",
      "epoch: 458  loss: 2.6140116538899747\n",
      "epoch: 459  loss: 2.6118380659481772\n",
      "epoch: 460  loss: 2.6098546300736416\n",
      "epoch: 461  loss: 2.6079339358229845\n",
      "epoch: 462  loss: 2.6058463236486205\n",
      "epoch: 463  loss: 2.6036492825742243\n",
      "epoch: 464  loss: 2.601774631387343\n",
      "epoch: 465  loss: 2.599631247951038\n",
      "epoch: 466  loss: 2.597712063066865\n",
      "epoch: 467  loss: 2.5956291810389303\n",
      "epoch: 468  loss: 2.593738461756402\n",
      "epoch: 469  loss: 2.5918034897304096\n",
      "epoch: 470  loss: 2.5895862113702606\n",
      "epoch: 471  loss: 2.5878632064832345\n",
      "epoch: 472  loss: 2.585767926810149\n",
      "epoch: 473  loss: 2.5839185200029533\n",
      "epoch: 474  loss: 2.581879647963433\n",
      "epoch: 475  loss: 2.579880462503752\n",
      "epoch: 476  loss: 2.578156566526559\n",
      "epoch: 477  loss: 2.5760434054827783\n",
      "epoch: 478  loss: 2.5740778838317055\n",
      "epoch: 479  loss: 2.5721468836463828\n",
      "epoch: 480  loss: 2.5702164460672066\n",
      "epoch: 481  loss: 2.5683112911019634\n",
      "epoch: 482  loss: 2.5663691688687322\n",
      "epoch: 483  loss: 2.5643442763293933\n",
      "epoch: 484  loss: 2.5625931951863095\n",
      "epoch: 485  loss: 2.5606144121202306\n",
      "epoch: 486  loss: 2.5586352084665123\n",
      "epoch: 487  loss: 2.556735753749308\n",
      "epoch: 488  loss: 2.5549292532396066\n",
      "epoch: 489  loss: 2.5530228924708354\n",
      "epoch: 490  loss: 2.5509793832061405\n",
      "epoch: 491  loss: 2.5491222133832707\n",
      "epoch: 492  loss: 2.5473304147135423\n",
      "epoch: 493  loss: 2.5454138699797113\n",
      "epoch: 494  loss: 2.5433591186638296\n",
      "epoch: 495  loss: 2.5414904916342493\n",
      "epoch: 496  loss: 2.5396564420707364\n",
      "epoch: 497  loss: 2.5377280915809024\n",
      "epoch: 498  loss: 2.535743784767874\n",
      "epoch: 499  loss: 2.533938400185434\n",
      "epoch: 500  loss: 2.532056514697615\n",
      "epoch: 501  loss: 2.530178722954588\n",
      "epoch: 502  loss: 2.5281713530048364\n",
      "epoch: 503  loss: 2.5263981967691507\n",
      "epoch: 504  loss: 2.52447665656382\n",
      "epoch: 505  loss: 2.5226419579566937\n",
      "epoch: 506  loss: 2.520718002659123\n",
      "epoch: 507  loss: 2.51888158991369\n",
      "epoch: 508  loss: 2.5169947374665753\n",
      "epoch: 509  loss: 2.5153796607160075\n",
      "epoch: 510  loss: 2.513410871822998\n",
      "epoch: 511  loss: 2.511448273622591\n",
      "epoch: 512  loss: 2.5095498898904225\n",
      "epoch: 513  loss: 2.507952337284678\n",
      "epoch: 514  loss: 2.5060452362640717\n",
      "epoch: 515  loss: 2.5041931690921047\n",
      "epoch: 516  loss: 2.50213272630117\n",
      "epoch: 517  loss: 2.5005601658895102\n",
      "epoch: 518  loss: 2.498817901732309\n",
      "epoch: 519  loss: 2.4967759359915362\n",
      "epoch: 520  loss: 2.4949172890092086\n",
      "epoch: 521  loss: 2.4931858575623664\n",
      "epoch: 522  loss: 2.4915663539909474\n",
      "epoch: 523  loss: 2.489679022265591\n",
      "epoch: 524  loss: 2.4876323652174506\n",
      "epoch: 525  loss: 2.4858775833863547\n",
      "epoch: 526  loss: 2.4842257653895103\n",
      "epoch: 527  loss: 2.4823504089508788\n",
      "epoch: 528  loss: 2.480377488207978\n",
      "epoch: 529  loss: 2.478628971012313\n",
      "epoch: 530  loss: 2.476872944238494\n",
      "epoch: 531  loss: 2.47527156133674\n",
      "epoch: 532  loss: 2.473431926259309\n",
      "epoch: 533  loss: 2.471463182676871\n",
      "epoch: 534  loss: 2.4697345795866568\n",
      "epoch: 535  loss: 2.468069299536637\n",
      "epoch: 536  loss: 2.466200579933229\n",
      "epoch: 537  loss: 2.4644293920246128\n",
      "epoch: 538  loss: 2.462534268284344\n",
      "epoch: 539  loss: 2.4608849799119525\n",
      "epoch: 540  loss: 2.459079171662779\n",
      "epoch: 541  loss: 2.4573088953511615\n",
      "epoch: 542  loss: 2.45546269509623\n",
      "epoch: 543  loss: 2.453776725852549\n",
      "epoch: 544  loss: 2.451913967267501\n",
      "epoch: 545  loss: 2.450325168862946\n",
      "epoch: 546  loss: 2.448454023613067\n",
      "epoch: 547  loss: 2.446624096834512\n",
      "epoch: 548  loss: 2.444931842740516\n",
      "epoch: 549  loss: 2.443130646335703\n",
      "epoch: 550  loss: 2.4415619341980346\n",
      "epoch: 551  loss: 2.439676908088586\n",
      "epoch: 552  loss: 2.4377306946107637\n",
      "epoch: 553  loss: 2.4361130802785738\n",
      "epoch: 554  loss: 2.43455301552558\n",
      "epoch: 555  loss: 2.432791295394509\n",
      "epoch: 556  loss: 2.430737860269346\n",
      "epoch: 557  loss: 2.4290979264910675\n",
      "epoch: 558  loss: 2.42750182307509\n",
      "epoch: 559  loss: 2.4260510197559597\n",
      "epoch: 560  loss: 2.423715349084432\n",
      "epoch: 561  loss: 2.4220618697167993\n",
      "epoch: 562  loss: 2.420261477128406\n",
      "epoch: 563  loss: 2.418949025247457\n",
      "epoch: 564  loss: 2.416880643269451\n",
      "epoch: 565  loss: 2.4150765380491066\n",
      "epoch: 566  loss: 2.413266679989192\n",
      "epoch: 567  loss: 2.411879755036807\n",
      "epoch: 568  loss: 2.410043230897827\n",
      "epoch: 569  loss: 2.4080646464572055\n",
      "epoch: 570  loss: 2.406294751117912\n",
      "epoch: 571  loss: 2.404823707194282\n",
      "epoch: 572  loss: 2.403405253065557\n",
      "epoch: 573  loss: 2.401367972001026\n",
      "epoch: 574  loss: 2.399407132476881\n",
      "epoch: 575  loss: 2.3978423895209744\n",
      "epoch: 576  loss: 2.396920562411651\n",
      "epoch: 577  loss: 2.3958075929326696\n",
      "epoch: 578  loss: 2.3930962829990676\n",
      "epoch: 579  loss: 2.39205401427148\n",
      "epoch: 580  loss: 2.390921023431474\n",
      "epoch: 581  loss: 2.3917086258097697\n",
      "epoch: 582  loss: 2.389045531190277\n",
      "epoch: 583  loss: 2.3885976032488543\n",
      "epoch: 584  loss: 2.3877412395290776\n",
      "epoch: 585  loss: 2.393978276884809\n",
      "epoch: 586  loss: 2.3888522349950563\n",
      "epoch: 587  loss: 2.3958162216106302\n",
      "epoch: 588  loss: 2.3955955660965174\n",
      "epoch: 589  loss: 2.4262791575906704\n",
      "epoch: 590  loss: 2.4201162233855484\n",
      "epoch: 591  loss: 2.4555699746963455\n",
      "epoch: 592  loss: 2.45457711940594\n",
      "epoch: 593  loss: 2.5717706078926312\n",
      "epoch: 594  loss: 2.533561737811624\n",
      "epoch: 595  loss: 2.707271793814243\n",
      "epoch: 596  loss: 2.614838188970907\n",
      "epoch: 597  loss: 2.6185440640379056\n",
      "epoch: 598  loss: 2.605746442600889\n",
      "epoch: 599  loss: 2.6009243298763067\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "pa_im = '/Users/david/Desktop/venv_enviorments/cmput414_labs/lab1/data/canoe/input/'\n",
    "ft_im = 'jpg'\n",
    "\n",
    "\n",
    "pa_gt = '/Users/david/Desktop/venv_enviorments/cmput414_labs/lab1/data/canoe/groundtruth/'\n",
    "ft_gt ='png'\n",
    "\n",
    "imgs = loadImgs_plus(pa_im, ft_im)\n",
    "gtimgs = loadImgs_plus(pa_gt, ft_gt)\n",
    "\n",
    "\n",
    "frame, row, column, byte = imgs.shape\n",
    "\n",
    "#use no.960\n",
    "crim = imgs.type(torch.FloatTensor)[960]\n",
    "bkim = imgs.type(torch.FloatTensor).mean(dim = 0)\n",
    "\n",
    "# merge \n",
    "# [240, 320,3] + [240, 320, 3]\n",
    "# rgb merged => [240, 320, 6]\n",
    "inputdata = torch.cat((crim, bkim), dim = 2)\n",
    "\n",
    "# [76800 * 6]\n",
    "inputdata = inputdata.permute(2, 0, 1).reshape(6, row*column).permute(1, 0)\n",
    "\n",
    "\n",
    "#use no.960 imgs in ground truth\n",
    "gtlabs = gtimgs.type(torch.int64)[960].reshape(row*column)/255\n",
    "gtlabs = gtlabs.type(torch.int64)\n",
    "\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "\n",
    "\n",
    "#TODO\n",
    "#network = BGNet().to(device)\n",
    "network = BGNet().to(device)\n",
    "\n",
    "optim_net = optim.SGD(network.parameters(), lr= 0.0001, momentum = 0.9)\n",
    "#optim_net = optim.Adam(network.parameters(), lr = 0.001)\n",
    "\n",
    "class_weights = torch.FloatTensor([0.5, 0.5]).to(device)\n",
    "loss_func = torch.nn.NLLLoss(weight=class_weights, reduction='mean').to(device)\n",
    "\n",
    "\n",
    "# get 1000 data, from 76800\n",
    "batchsize = 1000\n",
    "value = round(inputdata.shape[0]/batchsize + 0.5)\n",
    "\n",
    "# 600 original\n",
    "epochs = 600\n",
    "\n",
    "for i in range(epochs):\n",
    "\n",
    "    totalloss = 0\n",
    "    for j in range(value):\n",
    "        data = inputdata[j*batchsize:(j + 1)*batchsize].to(device)\n",
    "        labs = gtlabs[j*batchsize:(j + 1)*batchsize].to(device)\n",
    "\n",
    "        output = network(data)\n",
    "\n",
    "        loss = loss_func(output, labs)\n",
    "\n",
    "        totalloss = totalloss + loss.item()\n",
    "\n",
    "        optim_net.zero_grad()\n",
    "        loss.backward(retain_graph = True)\n",
    "        optim_net.step()\n",
    "\n",
    "    print(\"epoch:\", i, \" loss:\", totalloss)\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c30afa22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000e+00, -2.1840e+01],\n",
      "        [ 0.0000e+00, -1.9284e+01],\n",
      "        [ 0.0000e+00, -1.9642e+01],\n",
      "        ...,\n",
      "        [ 0.0000e+00, -1.8324e+01],\n",
      "        [-1.4305e-06, -1.3491e+01],\n",
      "        [-7.5102e-06, -1.1791e+01]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[-1.9073e-06, -1.3192e+01],\n",
      "        [-4.4941e-05, -1.0010e+01],\n",
      "        [-1.8596e-05, -1.0890e+01],\n",
      "        ...,\n",
      "        [ 0.0000e+00, -1.7620e+01],\n",
      "        [ 0.0000e+00, -1.7547e+01],\n",
      "        [ 0.0000e+00, -1.6854e+01]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[ 0.0000e+00, -1.7786e+01],\n",
      "        [ 0.0000e+00, -1.6980e+01],\n",
      "        [ 0.0000e+00, -1.7482e+01],\n",
      "        ...,\n",
      "        [-1.0729e-06, -1.3791e+01],\n",
      "        [-2.3842e-07, -1.5052e+01],\n",
      "        [-1.1921e-07, -1.5823e+01]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[ 0.0000e+00, -1.6762e+01],\n",
      "        [ 0.0000e+00, -1.7633e+01],\n",
      "        [ 0.0000e+00, -1.7768e+01],\n",
      "        ...,\n",
      "        [-2.0266e-06, -1.3109e+01],\n",
      "        [-1.4305e-06, -1.3425e+01],\n",
      "        [-7.1526e-07, -1.4086e+01]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[ 0.0000e+00, -1.7748e+01],\n",
      "        [ 0.0000e+00, -1.7543e+01],\n",
      "        [ 0.0000e+00, -1.8563e+01],\n",
      "        ...,\n",
      "        [ 0.0000e+00, -1.6648e+01],\n",
      "        [-1.1921e-07, -1.6361e+01],\n",
      "        [-1.4305e-06, -1.3447e+01]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[ 0.0000e+00, -1.8234e+01],\n",
      "        [ 0.0000e+00, -1.6818e+01],\n",
      "        [-1.1921e-07, -1.6435e+01],\n",
      "        ...,\n",
      "        [-8.3446e-07, -1.3981e+01],\n",
      "        [-2.3842e-07, -1.5072e+01],\n",
      "        [-1.1921e-07, -1.5667e+01]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[-1.4305e-06, -1.3477e+01],\n",
      "        [-4.7684e-07, -1.4626e+01],\n",
      "        [-5.9605e-07, -1.4250e+01],\n",
      "        ...,\n",
      "        [-1.6689e-06, -1.3333e+01],\n",
      "        [-4.7684e-07, -1.4685e+01],\n",
      "        [-8.3446e-07, -1.3941e+01]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[-1.0729e-06, -1.3753e+01],\n",
      "        [-1.3113e-06, -1.3517e+01],\n",
      "        [-2.3842e-07, -1.5221e+01],\n",
      "        ...,\n",
      "        [-2.3842e-07, -1.5230e+01],\n",
      "        [-3.5763e-07, -1.4730e+01],\n",
      "        [-5.9605e-07, -1.4420e+01]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[  0.0000, -17.3517],\n",
      "        [  0.0000, -17.3529],\n",
      "        [  0.0000, -17.3098],\n",
      "        ...,\n",
      "        [  0.0000, -19.5662],\n",
      "        [  0.0000, -19.9738],\n",
      "        [  0.0000, -20.3230]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[ 0.0000e+00, -2.0360e+01],\n",
      "        [ 0.0000e+00, -2.0019e+01],\n",
      "        [ 0.0000e+00, -1.9803e+01],\n",
      "        ...,\n",
      "        [-2.5987e-05, -1.0557e+01],\n",
      "        [-2.5034e-06, -1.2893e+01],\n",
      "        [-2.2650e-06, -1.3000e+01]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[ 0.0000e+00, -1.9738e+01],\n",
      "        [ 0.0000e+00, -1.9195e+01],\n",
      "        [-1.1921e-07, -1.5855e+01],\n",
      "        ...,\n",
      "        [ 0.0000e+00, -1.9976e+01],\n",
      "        [ 0.0000e+00, -1.9140e+01],\n",
      "        [ 0.0000e+00, -1.9143e+01]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[  0.0000, -18.9011],\n",
      "        [  0.0000, -19.7064],\n",
      "        [  0.0000, -18.4645],\n",
      "        ...,\n",
      "        [  0.0000, -19.3626],\n",
      "        [  0.0000, -18.9455],\n",
      "        [  0.0000, -19.1645]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[-2.3842e-07, -1.5126e+01],\n",
      "        [-2.3842e-07, -1.5127e+01],\n",
      "        [-3.5763e-07, -1.4917e+01],\n",
      "        ...,\n",
      "        [ 0.0000e+00, -1.7013e+01],\n",
      "        [ 0.0000e+00, -1.7206e+01],\n",
      "        [ 0.0000e+00, -1.7339e+01]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[-3.5763e-07, -1.4944e+01],\n",
      "        [-1.1921e-07, -1.5940e+01],\n",
      "        [-5.9605e-07, -1.4326e+01],\n",
      "        ...,\n",
      "        [-1.7140e-03, -6.3698e+00],\n",
      "        [-5.3881e-05, -9.8299e+00],\n",
      "        [-1.5020e-05, -1.1110e+01]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[-3.5285e-05, -1.0252e+01],\n",
      "        [-6.9141e-06, -1.1878e+01],\n",
      "        [-1.0657e-04, -9.1471e+00],\n",
      "        ...,\n",
      "        [-2.3842e-07, -1.5272e+01],\n",
      "        [-4.7684e-07, -1.4621e+01],\n",
      "        [-4.7684e-07, -1.4544e+01]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[-4.8876e-06, -1.2227e+01],\n",
      "        [-1.9073e-06, -1.3197e+01],\n",
      "        [-4.5299e-06, -1.2295e+01],\n",
      "        ...,\n",
      "        [-7.6294e-06, -1.1780e+01],\n",
      "        [-6.5565e-06, -1.1930e+01],\n",
      "        [-5.8412e-06, -1.2042e+01]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[  0.0000, -19.4872],\n",
      "        [  0.0000, -19.8645],\n",
      "        [  0.0000, -21.7623],\n",
      "        ...,\n",
      "        [  0.0000, -18.3568],\n",
      "        [  0.0000, -17.0431],\n",
      "        [  0.0000, -17.2033]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[ 0.0000e+00, -1.7624e+01],\n",
      "        [ 0.0000e+00, -1.7559e+01],\n",
      "        [ 0.0000e+00, -1.7061e+01],\n",
      "        ...,\n",
      "        [-1.1921e-07, -1.6580e+01],\n",
      "        [-1.1921e-07, -1.6430e+01],\n",
      "        [-1.1921e-07, -1.5706e+01]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[-1.1921e-07, -1.6422e+01],\n",
      "        [-1.1921e-07, -1.5781e+01],\n",
      "        [-1.1921e-07, -1.5982e+01],\n",
      "        ...,\n",
      "        [-9.1791e-06, -1.1597e+01],\n",
      "        [-3.5763e-07, -1.4842e+01],\n",
      "        [-1.1921e-07, -1.5845e+01]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[-2.5868e-05, -1.0563e+01],\n",
      "        [-1.9073e-05, -1.0870e+01],\n",
      "        [-1.0014e-05, -1.1508e+01],\n",
      "        ...,\n",
      "        [-8.3446e-07, -1.4061e+01],\n",
      "        [-7.1526e-07, -1.4072e+01],\n",
      "        [-7.1526e-07, -1.4073e+01]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[-1.1921e-05, -1.1334e+01],\n",
      "        [-5.4836e-06, -1.2107e+01],\n",
      "        [-4.2438e-05, -1.0069e+01],\n",
      "        ...,\n",
      "        [-7.2787e-04, -7.2257e+00],\n",
      "        [-3.3731e-04, -7.9946e+00],\n",
      "        [-6.9665e-04, -7.2696e+00]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[-6.2720e-04, -7.3745e+00],\n",
      "        [-3.4005e-03, -5.6855e+00],\n",
      "        [-1.1900e-03, -6.7344e+00],\n",
      "        ...,\n",
      "        [-1.0729e-06, -1.3725e+01],\n",
      "        [-1.4424e-05, -1.1147e+01],\n",
      "        [-7.1526e-07, -1.4089e+01]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[-4.4107e-06, -1.2334e+01],\n",
      "        [-8.3446e-07, -1.4040e+01],\n",
      "        [-3.5763e-07, -1.4967e+01],\n",
      "        ...,\n",
      "        [-5.0424e-05, -9.8942e+00],\n",
      "        [-5.6028e-06, -1.2103e+01],\n",
      "        [-2.8610e-06, -1.2767e+01]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[-2.3842e-07, -1.5029e+01],\n",
      "        [-3.5763e-07, -1.4994e+01],\n",
      "        [-3.5763e-07, -1.4925e+01],\n",
      "        ...,\n",
      "        [-1.6621e-03, -6.4005e+00],\n",
      "        [-1.2932e-03, -6.6513e+00],\n",
      "        [-2.5162e-03, -5.9863e+00]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[-9.5367e-07, -1.3925e+01],\n",
      "        [-8.3446e-07, -1.3943e+01],\n",
      "        [-8.3446e-07, -1.3968e+01],\n",
      "        ...,\n",
      "        [-1.4435e-04, -8.8433e+00],\n",
      "        [-1.0836e-04, -9.1304e+00],\n",
      "        [-9.4409e-05, -9.2676e+00]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[-1.6689e-05, -1.1002e+01],\n",
      "        [-7.9870e-06, -1.1735e+01],\n",
      "        [-1.3113e-05, -1.1238e+01],\n",
      "        ...,\n",
      "        [-4.2140e-03, -5.4715e+00],\n",
      "        [-9.3512e-04, -6.9753e+00],\n",
      "        [-4.5299e-06, -1.2293e+01]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[-3.6955e-06, -1.2521e+01],\n",
      "        [-3.9339e-06, -1.2459e+01],\n",
      "        [-5.3644e-06, -1.2147e+01],\n",
      "        ...,\n",
      "        [-3.1471e-05, -1.0365e+01],\n",
      "        [-2.6822e-05, -1.0526e+01],\n",
      "        [-9.4175e-06, -1.1567e+01]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[-2.9087e-05, -1.0443e+01],\n",
      "        [-2.8729e-05, -1.0458e+01],\n",
      "        [-2.9325e-05, -1.0438e+01],\n",
      "        ...,\n",
      "        [-1.0742e-03, -6.8367e+00],\n",
      "        [-3.8771e-04, -7.8553e+00],\n",
      "        [-5.0663e-05, -9.8898e+00]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[-7.5933e-05, -9.4853e+00],\n",
      "        [-7.7602e-05, -9.4642e+00],\n",
      "        [-8.1059e-05, -9.4206e+00],\n",
      "        ...,\n",
      "        [-2.0981e-05, -1.0769e+01],\n",
      "        [-1.9789e-05, -1.0831e+01],\n",
      "        [-1.9669e-05, -1.0835e+01]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[-1.1921e-07, -1.5942e+01],\n",
      "        [-1.1921e-07, -1.6196e+01],\n",
      "        [-1.1921e-07, -1.5539e+01],\n",
      "        ...,\n",
      "        [-1.9193e-05, -1.0859e+01],\n",
      "        [-2.1696e-05, -1.0739e+01],\n",
      "        [-2.1696e-05, -1.0740e+01]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[-8.9407e-06, -1.1621e+01],\n",
      "        [-8.9407e-06, -1.1622e+01],\n",
      "        [-9.0599e-06, -1.1616e+01],\n",
      "        ...,\n",
      "        [-6.6757e-06, -1.1915e+01],\n",
      "        [-7.9870e-06, -1.1737e+01],\n",
      "        [-2.6464e-05, -1.0542e+01]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[-3.1124e-03, -5.7739e+00],\n",
      "        [-2.9250e-04, -8.1373e+00],\n",
      "        [-2.8022e-04, -8.1801e+00],\n",
      "        ...,\n",
      "        [-2.5019e-04, -8.2934e+00],\n",
      "        [-2.4423e-04, -8.3177e+00],\n",
      "        [-1.5317e-04, -8.7842e+00]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[-2.0742e-05, -1.0781e+01],\n",
      "        [-1.8477e-05, -1.0900e+01],\n",
      "        [-3.1709e-05, -1.0358e+01],\n",
      "        ...,\n",
      "        [-1.1921e-06, -1.3629e+01],\n",
      "        [-3.8245e-03, -5.5682e+00],\n",
      "        [-5.4942e-02, -2.9288e+00]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[-0.0158, -4.1564],\n",
      "        [-0.2371, -1.5553],\n",
      "        [-0.2417, -1.5384],\n",
      "        ...,\n",
      "        [-1.8558, -0.1700],\n",
      "        [-1.1799, -0.3672],\n",
      "        [-0.7680, -0.6235]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[-4.6600e+00, -9.5115e-03],\n",
      "        [-3.1218e+00, -4.5077e-02],\n",
      "        [-3.3259e+00, -3.6602e-02],\n",
      "        ...,\n",
      "        [-1.0729e-05, -1.1443e+01],\n",
      "        [-1.3113e-06, -1.3505e+01],\n",
      "        [-2.3484e-05, -1.0660e+01]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[-9.9229e-03, -4.6179e+00],\n",
      "        [-3.0245e-02, -3.5135e+00],\n",
      "        [-6.3175e-03, -5.0676e+00],\n",
      "        ...,\n",
      "        [-4.1024e-03, -5.4982e+00],\n",
      "        [-1.3303e-04, -8.9248e+00],\n",
      "        [-2.0587e-02, -3.8934e+00]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[-3.9764e-02, -3.2446e+00],\n",
      "        [-1.6859e-03, -6.3863e+00],\n",
      "        [-3.7832e-02, -3.2935e+00],\n",
      "        ...,\n",
      "        [-2.6067e-02, -3.6601e+00],\n",
      "        [-3.0579e-01, -1.3339e+00],\n",
      "        [-2.2443e-02, -3.8080e+00]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[-6.5565e-06, -1.1929e+01],\n",
      "        [-1.9312e-05, -1.0857e+01],\n",
      "        [-7.9445e-04, -7.1382e+00],\n",
      "        ...,\n",
      "        [-2.9888e-03, -5.8144e+00],\n",
      "        [-1.0519e-01, -2.3041e+00],\n",
      "        [-3.2170e-02, -3.4528e+00]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[-7.7486e-06, -1.1765e+01],\n",
      "        [-5.6504e-05, -9.7814e+00],\n",
      "        [-1.5855e-05, -1.1051e+01],\n",
      "        ...,\n",
      "        [-1.3113e-06, -1.3536e+01],\n",
      "        [-1.6212e-05, -1.1029e+01],\n",
      "        [-1.1325e-05, -1.1388e+01]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[-1.5305e-04, -8.7852e+00],\n",
      "        [-4.8554e-04, -7.6304e+00],\n",
      "        [-9.9296e-05, -9.2174e+00],\n",
      "        ...,\n",
      "        [-3.5047e-05, -1.0259e+01],\n",
      "        [-4.6123e-04, -7.6819e+00],\n",
      "        [-1.1598e-04, -9.0620e+00]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[-1.1921e-05, -1.1337e+01],\n",
      "        [-1.4305e-05, -1.1154e+01],\n",
      "        [-2.0861e-05, -1.0778e+01],\n",
      "        ...,\n",
      "        [-2.3136e-04, -8.3719e+00],\n",
      "        [-6.0080e-05, -9.7195e+00],\n",
      "        [-4.9186e-04, -7.6177e+00]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[-2.4438e-05, -1.0618e+01],\n",
      "        [-2.5889e-04, -8.2594e+00],\n",
      "        [-1.0384e-02, -4.5726e+00],\n",
      "        ...,\n",
      "        [-1.4878e+00, -2.5601e-01],\n",
      "        [-1.5395e+00, -2.4143e-01],\n",
      "        [-1.6578e+00, -2.1140e-01]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[-1.7089e+00, -1.9975e-01],\n",
      "        [-1.9588e+00, -1.5203e-01],\n",
      "        [-1.9784e+00, -1.4883e-01],\n",
      "        ...,\n",
      "        [-2.0345e-03, -6.1985e+00],\n",
      "        [-9.7930e-04, -6.9292e+00],\n",
      "        [-1.8998e-03, -6.2669e+00]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[-1.1249e-03, -6.7906e+00],\n",
      "        [-4.4204e-03, -5.4237e+00],\n",
      "        [-4.1284e-03, -5.4919e+00],\n",
      "        ...,\n",
      "        [-3.7249e-01, -1.1680e+00],\n",
      "        [-1.1319e+00, -3.8923e-01],\n",
      "        [-1.6383e+00, -2.1605e-01]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[-7.5093e+00, -5.4809e-04],\n",
      "        [-4.8712e+00, -7.6935e-03],\n",
      "        [-2.9015e+00, -5.6508e-02],\n",
      "        ...,\n",
      "        [-3.8080e-02, -3.2871e+00],\n",
      "        [-6.5605e-02, -2.7567e+00],\n",
      "        [-7.8064e-02, -2.5890e+00]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[-5.3890e-02, -2.9476e+00],\n",
      "        [-1.0280e-01, -2.3259e+00],\n",
      "        [-6.1441e-02, -2.8202e+00],\n",
      "        ...,\n",
      "        [-9.2700e+00, -9.4171e-05],\n",
      "        [-9.0652e+00, -1.1563e-04],\n",
      "        [-9.1999e+00, -1.0108e-04]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[-4.0113, -0.0183],\n",
      "        [-4.0972, -0.0168],\n",
      "        [-4.2577, -0.0143],\n",
      "        ...,\n",
      "        [-3.5957, -0.0278],\n",
      "        [-3.2913, -0.0379],\n",
      "        [-4.1453, -0.0160]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[-4.2377e+00, -1.4546e-02],\n",
      "        [-4.0025e+00, -1.8439e-02],\n",
      "        [-3.9820e+00, -1.8824e-02],\n",
      "        ...,\n",
      "        [-2.3621e-03, -6.0494e+00],\n",
      "        [-2.0360e-03, -6.1978e+00],\n",
      "        [-8.2340e-04, -7.1025e+00]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[-0.0247, -3.7128],\n",
      "        [-0.0147, -4.2299],\n",
      "        [-0.0121, -4.4174],\n",
      "        ...,\n",
      "        [-0.0334, -3.4150],\n",
      "        [-0.0195, -3.9470],\n",
      "        [-0.0669, -2.7385]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[-3.9859e-03, -5.5270e+00],\n",
      "        [-4.4314e-03, -5.4213e+00],\n",
      "        [-4.4059e-03, -5.4270e+00],\n",
      "        ...,\n",
      "        [-1.0616e+00, -4.2450e-01],\n",
      "        [-1.1924e+00, -3.6166e-01],\n",
      "        [-6.2507e-01, -7.6620e-01]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[-7.6655e-02, -2.6065e+00],\n",
      "        [-7.7508e-02, -2.5959e+00],\n",
      "        [-5.3752e-02, -2.9501e+00],\n",
      "        ...,\n",
      "        [-5.1497e-05, -9.8743e+00],\n",
      "        [-2.9206e-05, -1.0441e+01],\n",
      "        [-2.3722e-05, -1.0649e+01]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[-1.2325e-04, -9.0012e+00],\n",
      "        [-7.4145e-05, -9.5101e+00],\n",
      "        [-1.2480e-04, -8.9893e+00],\n",
      "        ...,\n",
      "        [-2.5686e-04, -8.2671e+00],\n",
      "        [-2.5388e-04, -8.2789e+00],\n",
      "        [-2.5877e-04, -8.2597e+00]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[-7.5564e-03, -4.8891e+00],\n",
      "        [-6.1722e-03, -5.0908e+00],\n",
      "        [-6.4343e-03, -5.0493e+00],\n",
      "        ...,\n",
      "        [-6.2345e-05, -9.6824e+00],\n",
      "        [-9.3694e-05, -9.2757e+00],\n",
      "        [-1.2850e-04, -8.9599e+00]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[-8.2947e-04, -7.0952e+00],\n",
      "        [-1.3196e-04, -8.9333e+00],\n",
      "        [-2.5510e-05, -1.0576e+01],\n",
      "        ...,\n",
      "        [ 0.0000e+00, -1.6899e+01],\n",
      "        [-1.1921e-07, -1.5584e+01],\n",
      "        [-1.1921e-06, -1.3661e+01]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[-2.9802e-06, -1.2716e+01],\n",
      "        [-4.7684e-07, -1.4537e+01],\n",
      "        [-5.9605e-07, -1.4392e+01],\n",
      "        ...,\n",
      "        [-3.5763e-07, -1.4971e+01],\n",
      "        [ 0.0000e+00, -1.7251e+01],\n",
      "        [ 0.0000e+00, -1.7233e+01]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[-1.4543e-05, -1.1136e+01],\n",
      "        [-1.7285e-05, -1.0963e+01],\n",
      "        [-1.1921e-07, -1.5580e+01],\n",
      "        ...,\n",
      "        [-2.8610e-06, -1.2756e+01],\n",
      "        [-2.1458e-06, -1.3028e+01],\n",
      "        [-3.5763e-07, -1.4944e+01]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[-5.4836e-06, -1.2117e+01],\n",
      "        [-1.5378e-05, -1.1080e+01],\n",
      "        [-4.7684e-06, -1.2247e+01],\n",
      "        ...,\n",
      "        [-1.3351e-05, -1.1226e+01],\n",
      "        [-6.6757e-06, -1.1910e+01],\n",
      "        [-4.4107e-06, -1.2325e+01]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[-1.0610e-05, -1.1451e+01],\n",
      "        [-1.9073e-05, -1.0868e+01],\n",
      "        [-2.7060e-05, -1.0519e+01],\n",
      "        ...,\n",
      "        [-3.2186e-06, -1.2646e+01],\n",
      "        [-2.0027e-05, -1.0817e+01],\n",
      "        [-1.5974e-05, -1.1045e+01]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[-4.7684e-07, -1.4678e+01],\n",
      "        [-4.7684e-07, -1.4585e+01],\n",
      "        [-2.8610e-06, -1.2751e+01],\n",
      "        ...,\n",
      "        [-5.9605e-07, -1.4369e+01],\n",
      "        [-8.9407e-06, -1.1626e+01],\n",
      "        [-5.0068e-06, -1.2209e+01]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[-2.4438e-05, -1.0619e+01],\n",
      "        [-1.3196e-04, -8.9331e+00],\n",
      "        [-2.6106e-05, -1.0552e+01],\n",
      "        ...,\n",
      "        [-5.0068e-06, -1.2212e+01],\n",
      "        [-5.0663e-05, -9.8914e+00],\n",
      "        [-9.8820e-05, -9.2221e+00]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[-7.1526e-07, -1.4143e+01],\n",
      "        [-2.3842e-07, -1.5362e+01],\n",
      "        [-4.7684e-07, -1.4501e+01],\n",
      "        ...,\n",
      "        [-1.0610e-05, -1.1449e+01],\n",
      "        [-6.5205e-05, -9.6372e+00],\n",
      "        [-2.1577e-05, -1.0746e+01]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[-4.2915e-06, -1.2355e+01],\n",
      "        [-7.0464e-04, -7.2581e+00],\n",
      "        [-1.1789e-04, -9.0460e+00],\n",
      "        ...,\n",
      "        [-1.1921e-07, -1.5618e+01],\n",
      "        [-1.1921e-07, -1.5545e+01],\n",
      "        [-2.3842e-07, -1.5152e+01]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[-6.9141e-06, -1.1881e+01],\n",
      "        [-1.7524e-05, -1.0949e+01],\n",
      "        [-5.9604e-06, -1.2024e+01],\n",
      "        ...,\n",
      "        [-1.1253e-04, -9.0918e+00],\n",
      "        [-2.3720e-04, -8.3467e+00],\n",
      "        [-1.0907e-04, -9.1240e+00]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[-1.7341e-03, -6.3581e+00],\n",
      "        [-2.8952e-04, -8.1473e+00],\n",
      "        [-7.9445e-04, -7.1382e+00],\n",
      "        ...,\n",
      "        [-1.1921e-07, -1.6587e+01],\n",
      "        [ 0.0000e+00, -1.9120e+01],\n",
      "        [ 0.0000e+00, -1.9198e+01]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[-1.1921e-07, -1.6184e+01],\n",
      "        [-2.3842e-07, -1.5073e+01],\n",
      "        [-2.0504e-05, -1.0795e+01],\n",
      "        ...,\n",
      "        [ 0.0000e+00, -1.6853e+01],\n",
      "        [ 0.0000e+00, -1.7252e+01],\n",
      "        [-7.1526e-07, -1.4145e+01]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[-1.7881e-06, -1.3222e+01],\n",
      "        [-8.3446e-07, -1.3930e+01],\n",
      "        [ 0.0000e+00, -1.7004e+01],\n",
      "        ...,\n",
      "        [-2.3842e-06, -1.2952e+01],\n",
      "        [-3.3379e-06, -1.2616e+01],\n",
      "        [-6.1989e-06, -1.1994e+01]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[-8.1655e-05, -9.4124e+00],\n",
      "        [-1.5497e-06, -1.3365e+01],\n",
      "        [-1.1921e-07, -1.5670e+01],\n",
      "        ...,\n",
      "        [-4.7684e-07, -1.4640e+01],\n",
      "        [-1.1921e-07, -1.5721e+01],\n",
      "        [-2.3842e-07, -1.5448e+01]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[-9.5367e-07, -1.3879e+01],\n",
      "        [-3.4212e-05, -1.0283e+01],\n",
      "        [-5.4596e-05, -9.8157e+00],\n",
      "        ...,\n",
      "        [-5.9605e-07, -1.4328e+01],\n",
      "        [ 0.0000e+00, -1.6851e+01],\n",
      "        [-2.3842e-07, -1.5318e+01]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[-9.5367e-07, -1.3884e+01],\n",
      "        [-5.9605e-07, -1.4351e+01],\n",
      "        [-1.4305e-06, -1.3469e+01],\n",
      "        ...,\n",
      "        [-4.7684e-07, -1.4682e+01],\n",
      "        [-2.6226e-06, -1.2839e+01],\n",
      "        [-1.1921e-07, -1.5960e+01]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[-3.5763e-07, -1.4809e+01],\n",
      "        [-1.1921e-07, -1.6119e+01],\n",
      "        [-1.1921e-07, -1.6447e+01],\n",
      "        ...,\n",
      "        [-3.2186e-06, -1.2645e+01],\n",
      "        [-4.7684e-07, -1.4567e+01],\n",
      "        [-3.5763e-07, -1.4729e+01]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[-1.7881e-06, -1.3255e+01],\n",
      "        [-4.7684e-07, -1.4487e+01],\n",
      "        [-7.8678e-06, -1.1749e+01],\n",
      "        ...,\n",
      "        [-4.0531e-06, -1.2410e+01],\n",
      "        [-4.1723e-06, -1.2382e+01],\n",
      "        [-1.1348e-04, -9.0843e+00]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[-6.4848e-05, -9.6429e+00],\n",
      "        [-2.4438e-05, -1.0621e+01],\n",
      "        [-4.4107e-06, -1.2327e+01],\n",
      "        ...,\n",
      "        [ 0.0000e+00, -1.6795e+01],\n",
      "        [ 0.0000e+00, -1.8454e+01],\n",
      "        [ 0.0000e+00, -2.1349e+01]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[ 0.0000e+00, -1.6854e+01],\n",
      "        [-1.1921e-07, -1.6037e+01],\n",
      "        [-2.3842e-07, -1.5280e+01],\n",
      "        ...,\n",
      "        [-1.8595e-04, -8.5902e+00],\n",
      "        [-2.2385e-04, -8.4049e+00],\n",
      "        [-1.8595e-04, -8.5904e+00]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[-4.6231e-04, -7.6794e+00],\n",
      "        [-3.2224e-03, -5.7392e+00],\n",
      "        [-1.5177e-03, -6.4913e+00],\n",
      "        ...,\n",
      "        [-1.1921e-07, -1.6266e+01],\n",
      "        [-1.1921e-07, -1.6054e+01],\n",
      "        [-2.3842e-06, -1.2937e+01]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[-3.3259e-05, -1.0311e+01],\n",
      "        [-3.4809e-05, -1.0267e+01],\n",
      "        [-3.6955e-06, -1.2493e+01],\n",
      "        ...,\n",
      "        [-1.5782e-04, -8.7544e+00],\n",
      "        [-3.1828e-05, -1.0354e+01],\n",
      "        [-2.5391e-05, -1.0582e+01]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[-9.5367e-07, -1.3837e+01],\n",
      "        [-4.7684e-07, -1.4482e+01],\n",
      "        [-4.2915e-06, -1.2351e+01],\n",
      "        ...,\n",
      "        [-8.6542e-05, -9.3553e+00],\n",
      "        [-3.3855e-05, -1.0294e+01],\n",
      "        [-2.9802e-05, -1.0419e+01]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[-5.9722e-05, -9.7266e+00],\n",
      "        [-2.2325e-04, -8.4076e+00],\n",
      "        [-4.3276e-04, -7.7454e+00],\n",
      "        ...,\n",
      "        [-4.6492e-06, -1.2270e+01],\n",
      "        [-5.8412e-06, -1.2052e+01],\n",
      "        [-2.2650e-06, -1.2979e+01]], grad_fn=<LogSoftmaxBackward0>)\n",
      "Re: tensor(0.9562)  Pr: tensor(0.9895)  Fm: tensor(0.9726)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "prelabs = torch.abs(gtlabs - gtlabs)\n",
    "for j in range(value):\n",
    "    data = inputdata[j*batchsize:(j + 1)*batchsize].to(device)\n",
    "\n",
    "    output = network(data)\n",
    "    print(output)\n",
    "\n",
    "    prelabs[j*batchsize:(j + 1)*batchsize] = output.argmax(dim = 1)\n",
    "\n",
    "\n",
    "prefgim = prelabs.reshape(row, column)\n",
    "#gtim = gtimgs.type(torch.int64)[1000]\n",
    "gtim = gtimgs.type(torch.int64)[960]\n",
    "\n",
    "TP, FP, TN, FN = evaluation_entry(prefgim*255, gtim)\n",
    "\n",
    "Re = TP/(TP + FN)\n",
    "Pr = TP/(TP + FP)\n",
    "Fm = (2 * Pr * Re)/(Pr + Re)\n",
    "\n",
    "print(\"Re:\", Re, \" Pr:\", Pr, \" Fm:\", Fm)\n",
    "\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize = (4,4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(bkim.detach().cpu().numpy()/255.0)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(prefgim.detach().cpu().numpy())\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8694c534",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbe1359",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
