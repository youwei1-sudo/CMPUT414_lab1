{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2602a42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from dataIO import evaluation_entry\n",
    "\n",
    "\n",
    "from dataIO import loadImgs_plus\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1bb0850e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BGNet(nn.Module):\n",
    "    \n",
    "    # get 1000 1 6 1 shape\n",
    "    # predict forground or background\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # set stride , kernel size to 1 at the begining, and tune each layers\n",
    "        \n",
    "        # Conv layer 1\n",
    "        self.conv1 = nn.Conv2d(1,  16, (3, 1), stride = 1, padding = (1, 0))\n",
    "        self.b_norm1 = nn.BatchNorm2d(16)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(16, 32, (3, 1), stride = 1, padding = (0, 0))\n",
    "        self.b_norm2 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(32, 64, (1, 1), stride = 1, padding = (0, 0))\n",
    "        self.b_norm3 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(64, 128, (1, 1), stride = 1, padding = (0, 0))\n",
    "        self.b_norm4 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.conv5 = nn.Conv2d(128, 256, (1, 1), stride = 2, padding = (0, 0))\n",
    "        self.b_norm5 = nn.BatchNorm2d(256)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "        \n",
    "        # TODO\n",
    "        self.fc1 = nn.Linear(512,128)\n",
    "        self.fc2 = nn.Linear(128,64)\n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        self.fc4 = nn.Linear(32, 16)\n",
    "        self.fc5 = nn.Linear(16, 2)\n",
    "\n",
    "\n",
    "    def forward(self, inputdata):\n",
    "        \n",
    "        # 1000 1 6 1\n",
    "        #print(\"Inputdata shape: \",inputdata.shape)\n",
    "        \n",
    "        x = inputdata.unsqueeze(-1).unsqueeze(1)\n",
    "        x = self.conv1(x)\n",
    "        x = self.b_norm1(x)\n",
    "        x = F.relu(x)\n",
    "        #x = F.max_pool2d(x, 2, 2)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.b_norm2(x)\n",
    "        x = F.relu(x)\n",
    "        #x = F.max_pool2d(x, 2, 2)\n",
    "        #print(x.shape)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.b_norm3(x)\n",
    "        x = F.relu(x)\n",
    "        #x = F.max_pool2d(x, 2, 2)\n",
    "        \n",
    "        x = self.conv4(x)\n",
    "        x = self.b_norm4(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.conv5(x)\n",
    "        #print(\"conv5\", x.shape)\n",
    "        x = self.b_norm5(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = x.view(-1, 512) # 扁平化flat然后传入全连接层 ex [1000,256, 2, 1] => 256*2*1\n",
    "\n",
    "        x = F.relu(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.fc4(x)\n",
    "        \n",
    "        #random dropout\n",
    "        #x = self.dropout(x)\n",
    "        \n",
    "        x = self.fc5(x)\n",
    "        \n",
    "        \n",
    "        return F.log_softmax(x, dim = 1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "caac2327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading file: in001700.jpg\n",
      "loading file: gt001700.png\n",
      "epoch: 0  loss: 47.990385830402374\n",
      "epoch: 1  loss: 40.90608596801758\n",
      "epoch: 2  loss: 34.91574043035507\n",
      "epoch: 3  loss: 29.828808426856995\n",
      "epoch: 4  loss: 25.659960463643074\n",
      "epoch: 5  loss: 22.444495514035225\n",
      "epoch: 6  loss: 20.091177195310593\n",
      "epoch: 7  loss: 18.403392523527145\n",
      "epoch: 8  loss: 17.165518827736378\n",
      "epoch: 9  loss: 16.20709127932787\n",
      "epoch: 10  loss: 15.411112554371357\n",
      "epoch: 11  loss: 14.706602089107037\n",
      "epoch: 12  loss: 14.049968596547842\n",
      "epoch: 13  loss: 13.415879491716623\n",
      "epoch: 14  loss: 12.79892286285758\n",
      "epoch: 15  loss: 12.199957571923733\n",
      "epoch: 16  loss: 11.617296051234007\n",
      "epoch: 17  loss: 11.057292073965073\n",
      "epoch: 18  loss: 10.537140190601349\n",
      "epoch: 19  loss: 10.06393737345934\n",
      "epoch: 20  loss: 9.644206792116165\n",
      "epoch: 21  loss: 9.278580835089087\n",
      "epoch: 22  loss: 8.961421070620418\n",
      "epoch: 23  loss: 8.685381265357137\n",
      "epoch: 24  loss: 8.442765593528748\n",
      "epoch: 25  loss: 8.226428978145123\n",
      "epoch: 26  loss: 8.029446501284838\n",
      "epoch: 27  loss: 7.848002066835761\n",
      "epoch: 28  loss: 7.678891386836767\n",
      "epoch: 29  loss: 7.519004167988896\n",
      "epoch: 30  loss: 7.3672705590724945\n",
      "epoch: 31  loss: 7.222894735634327\n",
      "epoch: 32  loss: 7.085624041967094\n",
      "epoch: 33  loss: 6.9552285224199295\n",
      "epoch: 34  loss: 6.831329334527254\n",
      "epoch: 35  loss: 6.713820564560592\n",
      "epoch: 36  loss: 6.602203501388431\n",
      "epoch: 37  loss: 6.496177474036813\n",
      "epoch: 38  loss: 6.395412813872099\n",
      "epoch: 39  loss: 6.299705824814737\n",
      "epoch: 40  loss: 6.208712482824922\n",
      "epoch: 41  loss: 6.122325550764799\n",
      "epoch: 42  loss: 6.040349878370762\n",
      "epoch: 43  loss: 5.96221468411386\n",
      "epoch: 44  loss: 5.887857000343502\n",
      "epoch: 45  loss: 5.817265037447214\n",
      "epoch: 46  loss: 5.749878115020692\n",
      "epoch: 47  loss: 5.686056173406541\n",
      "epoch: 48  loss: 5.62517795804888\n",
      "epoch: 49  loss: 5.5672700153663754\n",
      "epoch: 50  loss: 5.512262572534382\n",
      "epoch: 51  loss: 5.459655133076012\n",
      "epoch: 52  loss: 5.409061691723764\n",
      "epoch: 53  loss: 5.36058968026191\n",
      "epoch: 54  loss: 5.313849509693682\n",
      "epoch: 55  loss: 5.269073963165283\n",
      "epoch: 56  loss: 5.2258893037214875\n",
      "epoch: 57  loss: 5.1839614296332\n",
      "epoch: 58  loss: 5.143564657308161\n",
      "epoch: 59  loss: 5.104504608083516\n",
      "epoch: 60  loss: 5.0667506833560765\n",
      "epoch: 61  loss: 5.029950025957078\n",
      "epoch: 62  loss: 4.994224803056568\n",
      "epoch: 63  loss: 4.959652174729854\n",
      "epoch: 64  loss: 4.925955647602677\n",
      "epoch: 65  loss: 4.893133060075343\n",
      "epoch: 66  loss: 4.86119471443817\n",
      "epoch: 67  loss: 4.830037633888423\n",
      "epoch: 68  loss: 4.79963991092518\n",
      "epoch: 69  loss: 4.769794624764472\n",
      "epoch: 70  loss: 4.740505805704743\n",
      "epoch: 71  loss: 4.711914679501206\n",
      "epoch: 72  loss: 4.683979350142181\n",
      "epoch: 73  loss: 4.6564528052695096\n",
      "epoch: 74  loss: 4.629677039105445\n",
      "epoch: 75  loss: 4.60351444222033\n",
      "epoch: 76  loss: 4.577682617586106\n",
      "epoch: 77  loss: 4.55235750367865\n",
      "epoch: 78  loss: 4.5274313651025295\n",
      "epoch: 79  loss: 4.5027626901865005\n",
      "epoch: 80  loss: 4.47816465748474\n",
      "epoch: 81  loss: 4.4539199732244015\n",
      "epoch: 82  loss: 4.429790956899524\n",
      "epoch: 83  loss: 4.405707443132997\n",
      "epoch: 84  loss: 4.381935562007129\n",
      "epoch: 85  loss: 4.358672535978258\n",
      "epoch: 86  loss: 4.33556114975363\n",
      "epoch: 87  loss: 4.312878333032131\n",
      "epoch: 88  loss: 4.290149805136025\n",
      "epoch: 89  loss: 4.2673916588537395\n",
      "epoch: 90  loss: 4.245170909445733\n",
      "epoch: 91  loss: 4.223224439658225\n",
      "epoch: 92  loss: 4.201594768092036\n",
      "epoch: 93  loss: 4.179881115909666\n",
      "epoch: 94  loss: 4.158632468432188\n",
      "epoch: 95  loss: 4.137547553051263\n",
      "epoch: 96  loss: 4.116581325884908\n",
      "epoch: 97  loss: 4.095795682165772\n",
      "epoch: 98  loss: 4.075281385332346\n",
      "epoch: 99  loss: 4.054705843096599\n",
      "epoch: 100  loss: 4.0344852902926505\n",
      "epoch: 101  loss: 4.0142939342185855\n",
      "epoch: 102  loss: 3.99431803682819\n",
      "epoch: 103  loss: 3.9741523552220315\n",
      "epoch: 104  loss: 3.9543952979147434\n",
      "epoch: 105  loss: 3.9346304482314736\n",
      "epoch: 106  loss: 3.915111693320796\n",
      "epoch: 107  loss: 3.8958550866227597\n",
      "epoch: 108  loss: 3.8767468750011176\n",
      "epoch: 109  loss: 3.8577110373880714\n",
      "epoch: 110  loss: 3.839186917291954\n",
      "epoch: 111  loss: 3.8208606217522174\n",
      "epoch: 112  loss: 3.8027915223501623\n",
      "epoch: 113  loss: 3.7845409302972257\n",
      "epoch: 114  loss: 3.766878397669643\n",
      "epoch: 115  loss: 3.7488309445325285\n",
      "epoch: 116  loss: 3.731269741198048\n",
      "epoch: 117  loss: 3.714015227975324\n",
      "epoch: 118  loss: 3.696663038106635\n",
      "epoch: 119  loss: 3.6795979938469827\n",
      "epoch: 120  loss: 3.6623409749008715\n",
      "epoch: 121  loss: 3.645601848838851\n",
      "epoch: 122  loss: 3.628723865142092\n",
      "epoch: 123  loss: 3.612386452499777\n",
      "epoch: 124  loss: 3.595974835101515\n",
      "epoch: 125  loss: 3.5796500931028277\n",
      "epoch: 126  loss: 3.56356553430669\n",
      "epoch: 127  loss: 3.547491532517597\n",
      "epoch: 128  loss: 3.53169838571921\n",
      "epoch: 129  loss: 3.5158033003099263\n",
      "epoch: 130  loss: 3.499819009564817\n",
      "epoch: 131  loss: 3.4844777651596814\n",
      "epoch: 132  loss: 3.469012773828581\n",
      "epoch: 133  loss: 3.453893286641687\n",
      "epoch: 134  loss: 3.4384900226723403\n",
      "epoch: 135  loss: 3.423453791299835\n",
      "epoch: 136  loss: 3.4084780677221715\n",
      "epoch: 137  loss: 3.3937865601619706\n",
      "epoch: 138  loss: 3.3791632239008322\n",
      "epoch: 139  loss: 3.3647016063332558\n",
      "epoch: 140  loss: 3.350489698816091\n",
      "epoch: 141  loss: 3.33606642333325\n",
      "epoch: 142  loss: 3.322097933269106\n",
      "epoch: 143  loss: 3.308136797044426\n",
      "epoch: 144  loss: 3.294186026440002\n",
      "epoch: 145  loss: 3.280475918087177\n",
      "epoch: 146  loss: 3.2668844165746123\n",
      "epoch: 147  loss: 3.25320987298619\n",
      "epoch: 148  loss: 3.239781327894889\n",
      "epoch: 149  loss: 3.226428153575398\n",
      "epoch: 150  loss: 3.2132388481404632\n",
      "epoch: 151  loss: 3.19996993127279\n",
      "epoch: 152  loss: 3.186937533901073\n",
      "epoch: 153  loss: 3.1737884152680635\n",
      "epoch: 154  loss: 3.160728308139369\n",
      "epoch: 155  loss: 3.1479599896119907\n",
      "epoch: 156  loss: 3.1352333332179114\n",
      "epoch: 157  loss: 3.122552844812162\n",
      "epoch: 158  loss: 3.110019351472147\n",
      "epoch: 159  loss: 3.097494160407223\n",
      "epoch: 160  loss: 3.0850042088422924\n",
      "epoch: 161  loss: 3.0726966571528465\n",
      "epoch: 162  loss: 3.0602615784155205\n",
      "epoch: 163  loss: 3.0484776963712648\n",
      "epoch: 164  loss: 3.036223891424015\n",
      "epoch: 165  loss: 3.0248047682689503\n",
      "epoch: 166  loss: 3.0130302138859406\n",
      "epoch: 167  loss: 3.001631375751458\n",
      "epoch: 168  loss: 2.9901505331508815\n",
      "epoch: 169  loss: 2.9789473173441365\n",
      "epoch: 170  loss: 2.968014892656356\n",
      "epoch: 171  loss: 2.9567586480407044\n",
      "epoch: 172  loss: 2.9461000001756474\n",
      "epoch: 173  loss: 2.934879245585762\n",
      "epoch: 174  loss: 2.924335088580847\n",
      "epoch: 175  loss: 2.9138280587503687\n",
      "epoch: 176  loss: 2.9032370559871197\n",
      "epoch: 177  loss: 2.8926296567660756\n",
      "epoch: 178  loss: 2.882302939891815\n",
      "epoch: 179  loss: 2.87196374963969\n",
      "epoch: 180  loss: 2.861838312121108\n",
      "epoch: 181  loss: 2.8513316426542588\n",
      "epoch: 182  loss: 2.8409833219484426\n",
      "epoch: 183  loss: 2.8312105808290653\n",
      "epoch: 184  loss: 2.82109328918159\n",
      "epoch: 185  loss: 2.81077345344238\n",
      "epoch: 186  loss: 2.8010974543867633\n",
      "epoch: 187  loss: 2.7908126041293144\n",
      "epoch: 188  loss: 2.781494304013904\n",
      "epoch: 189  loss: 2.7710454575135373\n",
      "epoch: 190  loss: 2.7616868760087527\n",
      "epoch: 191  loss: 2.7521555908606388\n",
      "epoch: 192  loss: 2.74247378250584\n",
      "epoch: 193  loss: 2.732974970829673\n",
      "epoch: 194  loss: 2.7234659917885438\n",
      "epoch: 195  loss: 2.713833934161812\n",
      "epoch: 196  loss: 2.7046890162164345\n",
      "epoch: 197  loss: 2.695681306475308\n",
      "epoch: 198  loss: 2.686399627418723\n",
      "epoch: 199  loss: 2.6778470246936195\n",
      "epoch: 200  loss: 2.668100044480525\n",
      "epoch: 201  loss: 2.6597582766553387\n",
      "epoch: 202  loss: 2.6509632585803047\n",
      "epoch: 203  loss: 2.6419581567752175\n",
      "epoch: 204  loss: 2.633634969301056\n",
      "epoch: 205  loss: 2.6245437468169257\n",
      "epoch: 206  loss: 2.6168684199801646\n",
      "epoch: 207  loss: 2.6077547408640385\n",
      "epoch: 208  loss: 2.6004329086863436\n",
      "epoch: 209  loss: 2.591073790565133\n",
      "epoch: 210  loss: 2.583459646208212\n",
      "epoch: 211  loss: 2.574893052049447\n",
      "epoch: 212  loss: 2.5673806230188347\n",
      "epoch: 213  loss: 2.559002600959502\n",
      "epoch: 214  loss: 2.5511556972633116\n",
      "epoch: 215  loss: 2.5434161132434383\n",
      "epoch: 216  loss: 2.535255740280263\n",
      "epoch: 217  loss: 2.527889409364434\n",
      "epoch: 218  loss: 2.520053885760717\n",
      "epoch: 219  loss: 2.5127462489472236\n",
      "epoch: 220  loss: 2.5050206802552566\n",
      "epoch: 221  loss: 2.4975743259419687\n",
      "epoch: 222  loss: 2.4901814383047167\n",
      "epoch: 223  loss: 2.4826392306713387\n",
      "epoch: 224  loss: 2.475269731337903\n",
      "epoch: 225  loss: 2.4682661727129016\n",
      "epoch: 226  loss: 2.4609955062915105\n",
      "epoch: 227  loss: 2.453803972894093\n",
      "epoch: 228  loss: 2.447030899260426\n",
      "epoch: 229  loss: 2.4399189775576815\n",
      "epoch: 230  loss: 2.43304483598331\n",
      "epoch: 231  loss: 2.426227262505563\n",
      "epoch: 232  loss: 2.4193194561521523\n",
      "epoch: 233  loss: 2.4128219346457627\n",
      "epoch: 234  loss: 2.406070478260517\n",
      "epoch: 235  loss: 2.399661014060257\n",
      "epoch: 236  loss: 2.3925332276849076\n",
      "epoch: 237  loss: 2.38642908303882\n",
      "epoch: 238  loss: 2.379456838039914\n",
      "epoch: 239  loss: 2.3735524410149083\n",
      "epoch: 240  loss: 2.3663550378987566\n",
      "epoch: 241  loss: 2.360914880351629\n",
      "epoch: 242  loss: 2.353265036625089\n",
      "epoch: 243  loss: 2.34841526375385\n",
      "epoch: 244  loss: 2.3404109531256836\n",
      "epoch: 245  loss: 2.335747884673765\n",
      "epoch: 246  loss: 2.327790552400984\n",
      "epoch: 247  loss: 2.323745484580286\n",
      "epoch: 248  loss: 2.3149881293065846\n",
      "epoch: 249  loss: 2.3128692485915963\n",
      "epoch: 250  loss: 2.3018408166826703\n",
      "epoch: 251  loss: 2.3023463355202693\n",
      "epoch: 252  loss: 2.289052541455021\n",
      "epoch: 253  loss: 2.2916311004955787\n",
      "epoch: 254  loss: 2.2765113055647817\n",
      "epoch: 255  loss: 2.281398286257172\n",
      "epoch: 256  loss: 2.2638780161505565\n",
      "epoch: 257  loss: 2.271942860155832\n",
      "epoch: 258  loss: 2.25039296966861\n",
      "epoch: 259  loss: 2.263574792043073\n",
      "epoch: 260  loss: 2.237131944624707\n",
      "epoch: 261  loss: 2.2565201003744733\n",
      "epoch: 262  loss: 2.2242689641425386\n",
      "epoch: 263  loss: 2.2491644747788087\n",
      "epoch: 264  loss: 2.2116300446214154\n",
      "epoch: 265  loss: 2.2440665020694723\n",
      "epoch: 266  loss: 2.199620376137318\n",
      "epoch: 267  loss: 2.240516482255771\n",
      "epoch: 268  loss: 2.188350293523399\n",
      "epoch: 269  loss: 2.238489422976272\n",
      "epoch: 270  loss: 2.179459534730995\n",
      "epoch: 271  loss: 2.237315688864328\n",
      "epoch: 272  loss: 2.1711321476323064\n",
      "epoch: 273  loss: 2.237806325996644\n",
      "epoch: 274  loss: 2.163533408049261\n",
      "epoch: 275  loss: 2.241050803946564\n",
      "epoch: 276  loss: 2.157906458218349\n",
      "epoch: 277  loss: 2.247166442874004\n",
      "epoch: 278  loss: 2.155199632194126\n",
      "epoch: 279  loss: 2.2537912260886515\n",
      "epoch: 280  loss: 2.152905119844945\n",
      "epoch: 281  loss: 2.264568346523447\n",
      "epoch: 282  loss: 2.155496509396471\n",
      "epoch: 283  loss: 2.2713963565183803\n",
      "epoch: 284  loss: 2.1559204551158473\n",
      "epoch: 285  loss: 2.264256790149375\n",
      "epoch: 286  loss: 2.1441431911080144\n",
      "epoch: 287  loss: 2.234234789517359\n",
      "epoch: 288  loss: 2.122134542791173\n",
      "epoch: 289  loss: 2.210906615408021\n",
      "epoch: 290  loss: 2.1112209495331626\n",
      "epoch: 291  loss: 2.20991910972225\n",
      "epoch: 292  loss: 2.1074628609931096\n",
      "epoch: 293  loss: 2.2075201369152637\n",
      "epoch: 294  loss: 2.1024225576547906\n",
      "epoch: 295  loss: 2.198266676510684\n",
      "epoch: 296  loss: 2.0925492591923103\n",
      "epoch: 297  loss: 2.184314934682334\n",
      "epoch: 298  loss: 2.082006982382154\n",
      "epoch: 299  loss: 2.172293934476329\n",
      "epoch: 300  loss: 2.071847190731205\n",
      "epoch: 301  loss: 2.1630092432606034\n",
      "epoch: 302  loss: 2.0653964067751076\n",
      "epoch: 303  loss: 2.1568926815816667\n",
      "epoch: 304  loss: 2.0556659496214706\n",
      "epoch: 305  loss: 2.149838550016284\n",
      "epoch: 306  loss: 2.053109441883862\n",
      "epoch: 307  loss: 2.1489693831681507\n",
      "epoch: 308  loss: 2.0437537373800296\n",
      "epoch: 309  loss: 2.1367480714106932\n",
      "epoch: 310  loss: 2.0387356159917545\n",
      "epoch: 311  loss: 2.1326632420532405\n",
      "epoch: 312  loss: 2.027623462723568\n",
      "epoch: 313  loss: 2.119322195649147\n",
      "epoch: 314  loss: 2.023559939363622\n",
      "epoch: 315  loss: 2.121511099889176\n",
      "epoch: 316  loss: 2.014662275512819\n",
      "epoch: 317  loss: 2.1055583482811926\n",
      "epoch: 318  loss: 2.008703645959031\n",
      "epoch: 319  loss: 2.098842205115943\n",
      "epoch: 320  loss: 1.9948836119729094\n",
      "epoch: 321  loss: 2.0799511300283484\n",
      "epoch: 322  loss: 1.9885508900333662\n",
      "epoch: 323  loss: 2.0769002145534614\n",
      "epoch: 324  loss: 1.9761004988540662\n",
      "epoch: 325  loss: 2.059660031736712\n",
      "epoch: 326  loss: 1.9708941207645694\n",
      "epoch: 327  loss: 2.0631463542958954\n",
      "epoch: 328  loss: 1.9659283887740457\n",
      "epoch: 329  loss: 2.0560285162500804\n",
      "epoch: 330  loss: 1.9630918759503402\n",
      "epoch: 331  loss: 2.051048096604063\n",
      "epoch: 332  loss: 1.9500919329439057\n",
      "epoch: 333  loss: 2.0354511902551167\n",
      "epoch: 334  loss: 1.9477915438037599\n",
      "epoch: 335  loss: 2.0383961581537733\n",
      "epoch: 336  loss: 1.9370722481544362\n",
      "epoch: 337  loss: 2.021640217237291\n",
      "epoch: 338  loss: 1.934419674653327\n",
      "epoch: 339  loss: 2.0224428528599674\n",
      "epoch: 340  loss: 1.9226163515122607\n",
      "epoch: 341  loss: 2.0055599490733584\n",
      "epoch: 342  loss: 1.9191099119343562\n",
      "epoch: 343  loss: 2.0065456917363917\n",
      "epoch: 344  loss: 1.9081707875739085\n",
      "epoch: 345  loss: 1.992471008328721\n",
      "epoch: 346  loss: 1.9065547150094062\n",
      "epoch: 347  loss: 1.9945906479260884\n",
      "epoch: 348  loss: 1.8963502001861343\n",
      "epoch: 349  loss: 1.9807760630501434\n",
      "epoch: 350  loss: 1.895075422944501\n",
      "epoch: 351  loss: 1.9856245967093855\n",
      "epoch: 352  loss: 1.8866915782709839\n",
      "epoch: 353  loss: 1.971392926585395\n",
      "epoch: 354  loss: 1.8848577646858757\n",
      "epoch: 355  loss: 1.9743202264653519\n",
      "epoch: 356  loss: 1.876703169895336\n",
      "epoch: 357  loss: 1.963185551794595\n",
      "epoch: 358  loss: 1.8752221551403636\n",
      "epoch: 359  loss: 1.967033653985709\n",
      "epoch: 360  loss: 1.870213069501915\n",
      "epoch: 361  loss: 1.959556851099478\n",
      "epoch: 362  loss: 1.8700603689503623\n",
      "epoch: 363  loss: 1.9574155293303193\n",
      "epoch: 364  loss: 1.859575721740839\n",
      "epoch: 365  loss: 1.9383340019194293\n",
      "epoch: 366  loss: 1.8520360258844448\n",
      "epoch: 367  loss: 1.936039191816235\n",
      "epoch: 368  loss: 1.8466524978721282\n",
      "epoch: 369  loss: 1.9337819882275653\n",
      "epoch: 370  loss: 1.8481314801465487\n",
      "epoch: 371  loss: 1.9336556483758613\n",
      "epoch: 372  loss: 1.8392969453561818\n",
      "epoch: 373  loss: 1.917163334073848\n",
      "epoch: 374  loss: 1.8291174634359777\n",
      "epoch: 375  loss: 1.9131880864224513\n",
      "epoch: 376  loss: 1.8267632414790569\n",
      "epoch: 377  loss: 1.911533988561132\n",
      "epoch: 378  loss: 1.822279555228306\n",
      "epoch: 379  loss: 1.9050728008223814\n",
      "epoch: 380  loss: 1.8192783978884108\n",
      "epoch: 381  loss: 1.9026470656972378\n",
      "epoch: 382  loss: 1.8128879814321408\n",
      "epoch: 383  loss: 1.8924503983653267\n",
      "epoch: 384  loss: 1.8074549277807819\n",
      "epoch: 385  loss: 1.8886923122627195\n",
      "epoch: 386  loss: 1.8010891735029873\n",
      "epoch: 387  loss: 1.882046377621009\n",
      "epoch: 388  loss: 1.7976214632653864\n",
      "epoch: 389  loss: 1.8800016071691061\n",
      "epoch: 390  loss: 1.7923337505562813\n",
      "epoch: 391  loss: 1.8714745475517702\n",
      "epoch: 392  loss: 1.7859642229159363\n",
      "epoch: 393  loss: 1.868604623065039\n",
      "epoch: 394  loss: 1.7839240692046587\n",
      "epoch: 395  loss: 1.8668556810880546\n",
      "epoch: 396  loss: 1.7779371871220064\n",
      "epoch: 397  loss: 1.858517462045711\n",
      "epoch: 398  loss: 1.7742486401548376\n",
      "epoch: 399  loss: 1.855024952266831\n",
      "epoch: 400  loss: 1.7676368394168094\n",
      "epoch: 401  loss: 1.8458487148673157\n",
      "epoch: 402  loss: 1.7654567420831881\n",
      "epoch: 403  loss: 1.8472548449863098\n",
      "epoch: 404  loss: 1.7606176803674316\n",
      "epoch: 405  loss: 1.8375907886729692\n",
      "epoch: 406  loss: 1.755131459365657\n",
      "epoch: 407  loss: 1.8339333777694264\n",
      "epoch: 408  loss: 1.749474446012755\n",
      "epoch: 409  loss: 1.829199548701581\n",
      "epoch: 410  loss: 1.749729286886577\n",
      "epoch: 411  loss: 1.827181749882584\n",
      "epoch: 412  loss: 1.740531473129522\n",
      "epoch: 413  loss: 1.8093227372664842\n",
      "epoch: 414  loss: 1.7307829315323033\n",
      "epoch: 415  loss: 1.8050021633825963\n",
      "epoch: 416  loss: 1.7294679917540634\n",
      "epoch: 417  loss: 1.8052101690191193\n",
      "epoch: 418  loss: 1.7232276607173844\n",
      "epoch: 419  loss: 1.7931660296962946\n",
      "epoch: 420  loss: 1.7148921610569232\n",
      "epoch: 421  loss: 1.7876246304585948\n",
      "epoch: 422  loss: 1.7135363492852775\n",
      "epoch: 423  loss: 1.7883183002268197\n",
      "epoch: 424  loss: 1.7091665952539188\n",
      "epoch: 425  loss: 1.7792772342581884\n",
      "epoch: 426  loss: 1.7065072578770923\n",
      "epoch: 427  loss: 1.7814807438699063\n",
      "epoch: 428  loss: 1.7050241091710632\n",
      "epoch: 429  loss: 1.7784703647630522\n",
      "epoch: 430  loss: 1.702508573216619\n",
      "epoch: 431  loss: 1.7753894252164173\n",
      "epoch: 432  loss: 1.6986552138187108\n",
      "epoch: 433  loss: 1.7700542551101535\n",
      "epoch: 434  loss: 1.6946296854148386\n",
      "epoch: 435  loss: 1.7665320856613107\n",
      "epoch: 436  loss: 1.690011994469387\n",
      "epoch: 437  loss: 1.7595251076709246\n",
      "epoch: 438  loss: 1.6846355443776702\n",
      "epoch: 439  loss: 1.752762736818113\n",
      "epoch: 440  loss: 1.6798446530956426\n",
      "epoch: 441  loss: 1.7490137919739936\n",
      "epoch: 442  loss: 1.6763442711016978\n",
      "epoch: 443  loss: 1.7456414619227871\n",
      "epoch: 444  loss: 1.6727026449298137\n",
      "epoch: 445  loss: 1.741451826026605\n",
      "epoch: 446  loss: 1.668927482656727\n",
      "epoch: 447  loss: 1.7369636109215207\n",
      "epoch: 448  loss: 1.6644711562330485\n",
      "epoch: 449  loss: 1.7313062926041312\n",
      "epoch: 450  loss: 1.659689358806645\n",
      "epoch: 451  loss: 1.7266429540104582\n",
      "epoch: 452  loss: 1.6557024873691262\n",
      "epoch: 453  loss: 1.7216520305519225\n",
      "epoch: 454  loss: 1.650596794715966\n",
      "epoch: 455  loss: 1.7160860738222254\n",
      "epoch: 456  loss: 1.6455006319229142\n",
      "epoch: 457  loss: 1.7115211816199007\n",
      "epoch: 458  loss: 1.6419662046027952\n",
      "epoch: 459  loss: 1.7073218846417149\n",
      "epoch: 460  loss: 1.6384711424107081\n",
      "epoch: 461  loss: 1.70313766131585\n",
      "epoch: 462  loss: 1.6344123303642846\n",
      "epoch: 463  loss: 1.696086716481659\n",
      "epoch: 464  loss: 1.6274067556296359\n",
      "epoch: 465  loss: 1.6855579780458356\n",
      "epoch: 466  loss: 1.6198658911162056\n",
      "epoch: 467  loss: 1.6775204140722053\n",
      "epoch: 468  loss: 1.615106276847655\n",
      "epoch: 469  loss: 1.6734285386301053\n",
      "epoch: 470  loss: 1.6091065002328833\n",
      "epoch: 471  loss: 1.6669956582190935\n",
      "epoch: 472  loss: 1.6048246482750983\n",
      "epoch: 473  loss: 1.6628126794748823\n",
      "epoch: 474  loss: 1.6012289011268876\n",
      "epoch: 475  loss: 1.6603077994150226\n",
      "epoch: 476  loss: 1.5985518666784628\n",
      "epoch: 477  loss: 1.6578406156804704\n",
      "epoch: 478  loss: 1.5960346890278743\n",
      "epoch: 479  loss: 1.654204275127995\n",
      "epoch: 480  loss: 1.5927616924382164\n",
      "epoch: 481  loss: 1.6506386723704054\n",
      "epoch: 482  loss: 1.5898810045036953\n",
      "epoch: 483  loss: 1.6477196795894997\n",
      "epoch: 484  loss: 1.586668880401703\n",
      "epoch: 485  loss: 1.6431293297173397\n",
      "epoch: 486  loss: 1.583035541101708\n",
      "epoch: 487  loss: 1.6412533511065703\n",
      "epoch: 488  loss: 1.579866286032484\n",
      "epoch: 489  loss: 1.6370074380938604\n",
      "epoch: 490  loss: 1.5776343759716838\n",
      "epoch: 491  loss: 1.636240327894484\n",
      "epoch: 492  loss: 1.5751461328109144\n",
      "epoch: 493  loss: 1.6333024165251118\n",
      "epoch: 494  loss: 1.5735787321973476\n",
      "epoch: 495  loss: 1.6322537991654826\n",
      "epoch: 496  loss: 1.571569854291738\n",
      "epoch: 497  loss: 1.6309631690928654\n",
      "epoch: 498  loss: 1.5688671847237856\n",
      "epoch: 499  loss: 1.6270546452942654\n",
      "epoch: 500  loss: 1.5662817767151864\n",
      "epoch: 501  loss: 1.6252702274250623\n",
      "epoch: 502  loss: 1.564522178930929\n",
      "epoch: 503  loss: 1.6224475996823458\n",
      "epoch: 504  loss: 1.5614293140533846\n",
      "epoch: 505  loss: 1.6177698615392728\n",
      "epoch: 506  loss: 1.558351766983833\n",
      "epoch: 507  loss: 1.6158440909421188\n",
      "epoch: 508  loss: 1.5559362888961914\n",
      "epoch: 509  loss: 1.6114005738854758\n",
      "epoch: 510  loss: 1.5522768402115616\n",
      "epoch: 511  loss: 1.606829812637443\n",
      "epoch: 512  loss: 1.5479375944923959\n",
      "epoch: 513  loss: 1.602432285581017\n",
      "epoch: 514  loss: 1.5440751954520238\n",
      "epoch: 515  loss: 1.5971733345868415\n",
      "epoch: 516  loss: 1.539801392103982\n",
      "epoch: 517  loss: 1.5927109427830146\n",
      "epoch: 518  loss: 1.5355130487732822\n",
      "epoch: 519  loss: 1.5866804519137077\n",
      "epoch: 520  loss: 1.5306306830461835\n",
      "epoch: 521  loss: 1.582592232622119\n",
      "epoch: 522  loss: 1.526784075627802\n",
      "epoch: 523  loss: 1.5792609753334546\n",
      "epoch: 524  loss: 1.5245889484795043\n",
      "epoch: 525  loss: 1.5761509984695294\n",
      "epoch: 526  loss: 1.5218372927738528\n",
      "epoch: 527  loss: 1.5746550180156191\n",
      "epoch: 528  loss: 1.520612765390979\n",
      "epoch: 529  loss: 1.5719609970255988\n",
      "epoch: 530  loss: 1.5186140719524701\n",
      "epoch: 531  loss: 1.568401130502025\n",
      "epoch: 532  loss: 1.5155232814504416\n",
      "epoch: 533  loss: 1.5645279459677113\n",
      "epoch: 534  loss: 1.5117406993849727\n",
      "epoch: 535  loss: 1.5606264744528744\n",
      "epoch: 536  loss: 1.508941602824052\n",
      "epoch: 537  loss: 1.5571304395271\n",
      "epoch: 538  loss: 1.5056972806087288\n",
      "epoch: 539  loss: 1.5529809530999046\n",
      "epoch: 540  loss: 1.503269442004239\n",
      "epoch: 541  loss: 1.5509207171708113\n",
      "epoch: 542  loss: 1.5012391199015838\n",
      "epoch: 543  loss: 1.5493514909321675\n",
      "epoch: 544  loss: 1.5000916919780138\n",
      "epoch: 545  loss: 1.547688358274172\n",
      "epoch: 546  loss: 1.4985478592425352\n",
      "epoch: 547  loss: 1.544081469066441\n",
      "epoch: 548  loss: 1.4959031978360144\n",
      "epoch: 549  loss: 1.5417933809258102\n",
      "epoch: 550  loss: 1.4937880172437872\n",
      "epoch: 551  loss: 1.5375295252524666\n",
      "epoch: 552  loss: 1.4890660712553654\n",
      "epoch: 553  loss: 1.534323286421568\n",
      "epoch: 554  loss: 1.4856566610069422\n",
      "epoch: 555  loss: 1.5295075920985255\n",
      "epoch: 556  loss: 1.4817710266906943\n",
      "epoch: 557  loss: 1.5248449058526603\n",
      "epoch: 558  loss: 1.4773077130485035\n",
      "epoch: 559  loss: 1.5196014483226463\n",
      "epoch: 560  loss: 1.4724072272474586\n",
      "epoch: 561  loss: 1.5131832779261458\n",
      "epoch: 562  loss: 1.4678212978215015\n",
      "epoch: 563  loss: 1.5095930110801419\n",
      "epoch: 564  loss: 1.4637203953025164\n",
      "epoch: 565  loss: 1.5041891852670233\n",
      "epoch: 566  loss: 1.4593359335049172\n",
      "epoch: 567  loss: 1.4988593205125653\n",
      "epoch: 568  loss: 1.4539764856053807\n",
      "epoch: 569  loss: 1.4930135991344287\n",
      "epoch: 570  loss: 1.4490390394275892\n",
      "epoch: 571  loss: 1.487677500314021\n",
      "epoch: 572  loss: 1.4446297856229648\n",
      "epoch: 573  loss: 1.4835793215788726\n",
      "epoch: 574  loss: 1.4409355492789473\n",
      "epoch: 575  loss: 1.4796914181824832\n",
      "epoch: 576  loss: 1.4373070209076104\n",
      "epoch: 577  loss: 1.4770764374516148\n",
      "epoch: 578  loss: 1.4351776435978536\n",
      "epoch: 579  loss: 1.4747819680342218\n",
      "epoch: 580  loss: 1.4330367409493192\n",
      "epoch: 581  loss: 1.473006202839315\n",
      "epoch: 582  loss: 1.4301429275910778\n",
      "epoch: 583  loss: 1.4699892661374179\n",
      "epoch: 584  loss: 1.4281558166185278\n",
      "epoch: 585  loss: 1.4677270350803155\n",
      "epoch: 586  loss: 1.4264689941628603\n",
      "epoch: 587  loss: 1.4670212715027446\n",
      "epoch: 588  loss: 1.425038806093653\n",
      "epoch: 589  loss: 1.46567169663831\n",
      "epoch: 590  loss: 1.4232106360250327\n",
      "epoch: 591  loss: 1.463145014749898\n",
      "epoch: 592  loss: 1.422288992365793\n",
      "epoch: 593  loss: 1.461674388316169\n",
      "epoch: 594  loss: 1.4189900280725851\n",
      "epoch: 595  loss: 1.458968642866239\n",
      "epoch: 596  loss: 1.4168068581384432\n",
      "epoch: 597  loss: 1.4558628579507058\n",
      "epoch: 598  loss: 1.4132356190530118\n",
      "epoch: 599  loss: 1.453680242640985\n",
      "BGNet(\n",
      "  (conv1): Conv2d(1, 16, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0))\n",
      "  (b_norm1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv2): Conv2d(16, 32, kernel_size=(3, 1), stride=(1, 1))\n",
      "  (b_norm2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv3): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (b_norm3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv4): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (b_norm4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv5): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2))\n",
      "  (b_norm5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dropout): Dropout(p=0.25, inplace=False)\n",
      "  (fc1): Linear(in_features=512, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (fc4): Linear(in_features=32, out_features=16, bias=True)\n",
      "  (fc5): Linear(in_features=16, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "pa_im = '/Users/david/Desktop/venv_enviorments/cmput414_labs/lab1/data/highway/input/'\n",
    "ft_im = 'jpg'\n",
    "\n",
    "\n",
    "pa_gt = '/Users/david/Desktop/venv_enviorments/cmput414_labs/lab1/data/highway/groundtruth/'\n",
    "ft_gt ='png'\n",
    "\n",
    "imgs = loadImgs_plus(pa_im, ft_im)\n",
    "gtimgs = loadImgs_plus(pa_gt, ft_gt)\n",
    "\n",
    "\n",
    "frame, row, column, byte = imgs.shape\n",
    "\n",
    "crim = imgs.type(torch.FloatTensor)[1000]\n",
    "bkim = imgs.type(torch.FloatTensor).mean(dim = 0)\n",
    "\n",
    "\n",
    "inputdata = torch.cat((crim, bkim), dim = 2)\n",
    "inputdata = inputdata.permute(2, 0, 1).reshape(6, row*column).permute(1, 0)\n",
    "\n",
    "gtlabs = gtimgs.type(torch.int64)[1000].reshape(row*column)/255\n",
    "gtlabs = gtlabs.type(torch.int64)\n",
    "\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "\n",
    "\n",
    "#TODO\n",
    "#network = BGNet().to(device)\n",
    "network = BGNet().to(device)\n",
    "\n",
    "optim_net = optim.SGD(network.parameters(), lr= 0.0001, momentum = 0.9)\n",
    "#optim_net = optim.Adam(network.parameters(), lr = 0.0001)\n",
    "\n",
    "class_weights = torch.FloatTensor([0.5, 0.5]).to(device)\n",
    "loss_func = torch.nn.NLLLoss(weight=class_weights, reduction='mean').to(device)\n",
    "\n",
    "\n",
    "\n",
    "batchsize = 1000\n",
    "value = round(inputdata.shape[0]/batchsize + 0.5)\n",
    "\n",
    "# 600 original\n",
    "epochs = 600\n",
    "\n",
    "for i in range(epochs):\n",
    "\n",
    "    totalloss = 0\n",
    "    for j in range(value):\n",
    "        data = inputdata[j*batchsize:(j + 1)*batchsize].to(device)\n",
    "        labs = gtlabs[j*batchsize:(j + 1)*batchsize].to(device)\n",
    "\n",
    "        output = network(data)\n",
    "\n",
    "        loss = loss_func(output, labs)\n",
    "\n",
    "        totalloss = totalloss + loss.item()\n",
    "\n",
    "        optim_net.zero_grad()   # 梯度归零\n",
    "        loss.backward(retain_graph = True)\n",
    "        optim_net.step() # update gradient\n",
    "\n",
    "    print(\"epoch:\", i, \" loss:\", totalloss)\n",
    "\n",
    "    \n",
    "    \n",
    "# my test\n",
    "# TODO\n",
    "input_test = torch.rand((1000, 6))\n",
    "net = BGNet()\n",
    "net.forward(input_test)\n",
    "print(net)  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f3f7c08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-4.7921e-05, -9.9460e+00],\n",
      "        [ 0.0000e+00, -1.8911e+01],\n",
      "        [-2.3842e-07, -1.5041e+01],\n",
      "        ...,\n",
      "        [-2.3842e-07, -1.5424e+01],\n",
      "        [-2.3842e-07, -1.5408e+01],\n",
      "        [-1.1921e-07, -1.5546e+01]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[-7.1526e-07, -1.4223e+01],\n",
      "        [ 0.0000e+00, -1.6845e+01],\n",
      "        [-1.1921e-07, -1.5666e+01],\n",
      "        ...,\n",
      "        [-1.1921e-07, -1.6459e+01],\n",
      "        [-2.3842e-07, -1.5280e+01],\n",
      "        [-7.1526e-07, -1.4211e+01]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[-1.1921e-07, -1.6112e+01],\n",
      "        [-1.1921e-07, -1.5768e+01],\n",
      "        [-1.1921e-07, -1.6335e+01],\n",
      "        ...,\n",
      "        [-2.3842e-06, -1.2954e+01],\n",
      "        [ 0.0000e+00, -1.7759e+01],\n",
      "        [-1.1921e-07, -1.5694e+01]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[ 0.0000e+00, -1.6678e+01],\n",
      "        [ 0.0000e+00, -1.6938e+01],\n",
      "        [ 0.0000e+00, -1.8435e+01],\n",
      "        ...,\n",
      "        [-5.3332e-04, -7.5367e+00],\n",
      "        [-1.4424e-05, -1.1149e+01],\n",
      "        [-7.0927e-05, -9.5533e+00]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[-8.5186e-04, -7.0685e+00],\n",
      "        [-8.0344e-05, -9.4297e+00],\n",
      "        [-4.8876e-06, -1.2220e+01],\n",
      "        ...,\n",
      "        [-6.4373e-06, -1.1957e+01],\n",
      "        [-2.3842e-07, -1.5530e+01],\n",
      "        [-2.9802e-06, -1.2737e+01]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[-6.6757e-06, -1.1921e+01],\n",
      "        [-1.0490e-05, -1.1469e+01],\n",
      "        [-4.7563e-05, -9.9537e+00],\n",
      "        ...,\n",
      "        [-9.9654e-05, -9.2144e+00],\n",
      "        [-4.0888e-05, -1.0104e+01],\n",
      "        [-4.6252e-05, -9.9815e+00]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[-7.6053e-05, -9.4843e+00],\n",
      "        [-2.9536e-04, -8.1274e+00],\n",
      "        [-2.5782e-03, -5.9620e+00],\n",
      "        ...,\n",
      "        [-1.5237e-02, -4.1917e+00],\n",
      "        [-1.0331e-03, -6.8756e+00],\n",
      "        [-1.2384e-03, -6.6945e+00]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[-1.6080e-04, -8.7355e+00],\n",
      "        [-7.0023e-04, -7.2644e+00],\n",
      "        [-6.7795e-04, -7.2968e+00],\n",
      "        ...,\n",
      "        [-3.5101e-02, -3.3670e+00],\n",
      "        [-9.8193e-02, -2.3695e+00],\n",
      "        [-3.2395e-02, -3.4459e+00]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[-1.1921e-06, -1.3658e+01],\n",
      "        [ 0.0000e+00, -1.6777e+01],\n",
      "        [-3.5763e-07, -1.4989e+01],\n",
      "        ...,\n",
      "        [-1.1921e-07, -1.5894e+01],\n",
      "        [ 0.0000e+00, -1.8493e+01],\n",
      "        [-6.7949e-06, -1.1899e+01]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[-3.5763e-07, -1.4810e+01],\n",
      "        [ 0.0000e+00, -1.6777e+01],\n",
      "        [-7.1526e-07, -1.4158e+01],\n",
      "        ...,\n",
      "        [ 0.0000e+00, -1.8661e+01],\n",
      "        [-2.3842e-06, -1.2970e+01],\n",
      "        [ 0.0000e+00, -1.6924e+01]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[ 0.0000e+00, -1.9677e+01],\n",
      "        [ 0.0000e+00, -1.8059e+01],\n",
      "        [-1.1921e-07, -1.6241e+01],\n",
      "        ...,\n",
      "        [-5.9605e-07, -1.4256e+01],\n",
      "        [-7.1526e-07, -1.4152e+01],\n",
      "        [-1.1921e-07, -1.5725e+01]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[-5.2093e-05, -9.8615e+00],\n",
      "        [-1.1921e-07, -1.5975e+01],\n",
      "        [-1.7881e-06, -1.3244e+01],\n",
      "        ...,\n",
      "        [-1.5497e-06, -1.3378e+01],\n",
      "        [-8.1062e-06, -1.1729e+01],\n",
      "        [-5.9605e-07, -1.4339e+01]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[-2.8610e-06, -1.2783e+01],\n",
      "        [-2.7418e-06, -1.2806e+01],\n",
      "        [-7.7364e-05, -9.4672e+00],\n",
      "        ...,\n",
      "        [-2.3351e+00, -1.0182e-01],\n",
      "        [-1.2286e+00, -3.4630e-01],\n",
      "        [-4.3520e+00, -1.2965e-02]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[-4.5475e-01, -1.0068e+00],\n",
      "        [-6.7703e-01, -7.0953e-01],\n",
      "        [-1.0263e+00, -4.4370e-01],\n",
      "        ...,\n",
      "        [-8.6341e+00, -1.7796e-04],\n",
      "        [-8.5987e+00, -1.8440e-04],\n",
      "        [-8.7964e+00, -1.5127e-04]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[-1.0534e+01, -2.6583e-05],\n",
      "        [-1.0246e+01, -3.5524e-05],\n",
      "        [-1.0242e+01, -3.5643e-05],\n",
      "        ...,\n",
      "        [-3.5640e-02, -3.3521e+00],\n",
      "        [-3.4575e-02, -3.3819e+00],\n",
      "        [-2.3100e-04, -8.3731e+00]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[-1.4733e-04, -8.8225e+00],\n",
      "        [-2.8749e-04, -8.1546e+00],\n",
      "        [-1.5166e-03, -6.4920e+00],\n",
      "        ...,\n",
      "        [-9.3813e-05, -9.2740e+00],\n",
      "        [-7.1526e-07, -1.4101e+01],\n",
      "        [-4.7684e-07, -1.4530e+01]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[-2.8976e-04, -8.1468e+00],\n",
      "        [-4.1961e-05, -1.0078e+01],\n",
      "        [-1.1921e-07, -1.5762e+01],\n",
      "        ...,\n",
      "        [ 0.0000e+00, -1.6652e+01],\n",
      "        [ 0.0000e+00, -1.7863e+01],\n",
      "        [ 0.0000e+00, -1.7670e+01]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[-1.1921e-07, -1.5744e+01],\n",
      "        [-4.7684e-07, -1.4484e+01],\n",
      "        [-5.0068e-06, -1.2216e+01],\n",
      "        ...,\n",
      "        [ 0.0000e+00, -1.7030e+01],\n",
      "        [-3.5763e-07, -1.4810e+01],\n",
      "        [-9.5367e-07, -1.3904e+01]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[ 0.0000e+00, -1.6754e+01],\n",
      "        [-5.9605e-07, -1.4323e+01],\n",
      "        [-2.3052e-04, -8.3752e+00],\n",
      "        ...,\n",
      "        [-1.1802e-05, -1.1342e+01],\n",
      "        [-2.9802e-06, -1.2706e+01],\n",
      "        [-4.7684e-06, -1.2260e+01]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[-7.7486e-06, -1.1762e+01],\n",
      "        [-1.2398e-05, -1.1299e+01],\n",
      "        [-2.2650e-06, -1.2988e+01],\n",
      "        ...,\n",
      "        [-5.1378e-05, -9.8755e+00],\n",
      "        [-7.5050e-04, -7.1952e+00],\n",
      "        [-2.9771e-03, -5.8183e+00]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[-6.7724e-04, -7.2979e+00],\n",
      "        [-1.4503e-03, -6.5367e+00],\n",
      "        [-1.1925e-03, -6.7323e+00],\n",
      "        ...,\n",
      "        [-3.4982e-04, -7.9584e+00],\n",
      "        [-2.5269e-04, -8.2833e+00],\n",
      "        [-2.2802e-04, -8.3860e+00]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[-8.7257e-05, -9.3464e+00],\n",
      "        [-1.0466e-04, -9.1649e+00],\n",
      "        [-1.4137e-04, -8.8646e+00],\n",
      "        ...,\n",
      "        [-3.4815e-04, -7.9632e+00],\n",
      "        [-4.4062e-04, -7.7276e+00],\n",
      "        [-5.0079e-04, -7.5996e+00]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[-2.9921e-05, -1.0415e+01],\n",
      "        [-2.8610e-05, -1.0460e+01],\n",
      "        [-2.7299e-05, -1.0511e+01],\n",
      "        ...,\n",
      "        [-3.5303e-03, -5.6481e+00],\n",
      "        [-1.2517e-05, -1.1292e+01],\n",
      "        [-4.7684e-07, -1.4590e+01]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[-4.1722e-05, -1.0085e+01],\n",
      "        [-6.1391e-05, -9.6977e+00],\n",
      "        [-1.5806e-04, -8.7528e+00],\n",
      "        ...,\n",
      "        [-7.2357e-05, -9.5342e+00],\n",
      "        [-1.8902e-03, -6.2721e+00],\n",
      "        [-2.0584e-03, -6.1869e+00]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[-1.1921e-07, -1.6542e+01],\n",
      "        [-2.0623e-05, -1.0787e+01],\n",
      "        [-1.1921e-07, -1.6017e+01],\n",
      "        ...,\n",
      "        [ 0.0000e+00, -1.8476e+01],\n",
      "        [-1.1921e-07, -1.6611e+01],\n",
      "        [ 0.0000e+00, -1.6753e+01]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[-1.1921e-07, -1.6475e+01],\n",
      "        [-4.7684e-07, -1.4667e+01],\n",
      "        [-1.1921e-07, -1.5762e+01],\n",
      "        ...,\n",
      "        [-3.0040e-05, -1.0414e+01],\n",
      "        [-3.2186e-06, -1.2658e+01],\n",
      "        [-5.8412e-06, -1.2051e+01]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[-1.1921e-07, -1.5586e+01],\n",
      "        [-1.1921e-07, -1.6227e+01],\n",
      "        [-6.1989e-06, -1.1990e+01],\n",
      "        ...,\n",
      "        [-2.3842e-07, -1.5242e+01],\n",
      "        [-4.7684e-07, -1.4477e+01],\n",
      "        [-7.1526e-07, -1.4179e+01]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[ 0.0000e+00, -1.7370e+01],\n",
      "        [ 0.0000e+00, -1.6979e+01],\n",
      "        [-2.3842e-07, -1.5132e+01],\n",
      "        ...,\n",
      "        [-3.3379e-06, -1.2609e+01],\n",
      "        [-2.9802e-06, -1.2707e+01],\n",
      "        [-2.7418e-06, -1.2802e+01]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[-2.6226e-06, -1.2844e+01],\n",
      "        [-2.6226e-06, -1.2843e+01],\n",
      "        [-2.7418e-06, -1.2818e+01],\n",
      "        ...,\n",
      "        [ 0.0000e+00, -1.9062e+01],\n",
      "        [ 0.0000e+00, -1.7246e+01],\n",
      "        [-9.7985e-05, -9.2313e+00]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[-1.6212e-05, -1.1028e+01],\n",
      "        [-9.4175e-06, -1.1569e+01],\n",
      "        [-8.1062e-06, -1.1729e+01],\n",
      "        ...,\n",
      "        [-1.0729e-06, -1.3773e+01],\n",
      "        [-4.7684e-07, -1.4661e+01],\n",
      "        [-2.3842e-07, -1.5477e+01]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[-3.5763e-07, -1.4695e+01],\n",
      "        [-3.5763e-07, -1.4765e+01],\n",
      "        [-3.5763e-07, -1.4772e+01],\n",
      "        ...,\n",
      "        [-3.6185e-04, -7.9244e+00],\n",
      "        [-4.8398e-05, -9.9354e+00],\n",
      "        [-7.1526e-07, -1.4224e+01]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[-2.3842e-06, -1.2968e+01],\n",
      "        [-2.3842e-07, -1.5461e+01],\n",
      "        [-1.1921e-07, -1.5858e+01],\n",
      "        ...,\n",
      "        [-1.0729e-06, -1.3732e+01],\n",
      "        [-6.7949e-06, -1.1896e+01],\n",
      "        [-9.5367e-07, -1.3812e+01]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[  0.0000, -19.2348],\n",
      "        [  0.0000, -19.2254],\n",
      "        [  0.0000, -19.1049],\n",
      "        ...,\n",
      "        [  0.0000, -18.4184],\n",
      "        [  0.0000, -17.2239],\n",
      "        [  0.0000, -17.1166]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[ 0.0000e+00, -1.9613e+01],\n",
      "        [ 0.0000e+00, -1.8914e+01],\n",
      "        [-1.1921e-07, -1.5983e+01],\n",
      "        ...,\n",
      "        [ 0.0000e+00, -1.6817e+01],\n",
      "        [ 0.0000e+00, -1.6709e+01],\n",
      "        [ 0.0000e+00, -1.6651e+01]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[ 0.0000e+00, -1.7149e+01],\n",
      "        [ 0.0000e+00, -1.7636e+01],\n",
      "        [ 0.0000e+00, -1.7384e+01],\n",
      "        ...,\n",
      "        [-8.3817e-04, -7.0847e+00],\n",
      "        [-4.6350e-04, -7.6768e+00],\n",
      "        [-2.6723e-04, -8.2277e+00]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[-5.9289e-04, -7.4307e+00],\n",
      "        [-4.7875e-04, -7.6445e+00],\n",
      "        [-3.6888e-04, -7.9053e+00],\n",
      "        ...,\n",
      "        [-2.3842e-06, -1.2941e+01],\n",
      "        [-1.4305e-06, -1.3448e+01],\n",
      "        [-1.1921e-06, -1.3655e+01]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[-1.6689e-06, -1.3331e+01],\n",
      "        [-1.3113e-06, -1.3559e+01],\n",
      "        [-1.1921e-06, -1.3645e+01],\n",
      "        ...,\n",
      "        [-3.3379e-06, -1.2598e+01],\n",
      "        [-2.3842e-06, -1.2942e+01],\n",
      "        [-1.5497e-06, -1.3380e+01]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[-1.4305e-06, -1.3454e+01],\n",
      "        [-1.0729e-06, -1.3704e+01],\n",
      "        [-7.1526e-07, -1.4075e+01],\n",
      "        ...,\n",
      "        [ 0.0000e+00, -1.6981e+01],\n",
      "        [ 0.0000e+00, -1.7234e+01],\n",
      "        [ 0.0000e+00, -1.7337e+01]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[-1.1921e-07, -1.6069e+01],\n",
      "        [-1.1921e-07, -1.6233e+01],\n",
      "        [-1.1921e-07, -1.6533e+01],\n",
      "        ...,\n",
      "        [-2.3779e-04, -8.3444e+00],\n",
      "        [-3.5232e-04, -7.9510e+00],\n",
      "        [-2.3246e-05, -1.0668e+01]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[-2.6464e-05, -1.0538e+01],\n",
      "        [-4.1723e-06, -1.2377e+01],\n",
      "        [-2.3603e-05, -1.0653e+01],\n",
      "        ...,\n",
      "        [-3.5763e-07, -1.4959e+01],\n",
      "        [-3.5763e-07, -1.4955e+01],\n",
      "        [-4.7684e-07, -1.4504e+01]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[ 0.0000e+00, -1.8865e+01],\n",
      "        [-2.3842e-07, -1.5386e+01],\n",
      "        [ 0.0000e+00, -1.9639e+01],\n",
      "        ...,\n",
      "        [-5.9605e-07, -1.4241e+01],\n",
      "        [-1.1921e-07, -1.6260e+01],\n",
      "        [ 0.0000e+00, -1.7622e+01]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[ 0.0000e+00, -1.7681e+01],\n",
      "        [ 0.0000e+00, -1.8458e+01],\n",
      "        [ 0.0000e+00, -1.8342e+01],\n",
      "        ...,\n",
      "        [-3.5763e-07, -1.4791e+01],\n",
      "        [-1.8358e-05, -1.0906e+01],\n",
      "        [-8.8807e-05, -9.3296e+00]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[-1.6700e-04, -8.6973e+00],\n",
      "        [-2.5007e-04, -8.2938e+00],\n",
      "        [-2.5436e-04, -8.2768e+00],\n",
      "        ...,\n",
      "        [-4.7961e+00, -8.2958e-03],\n",
      "        [-3.7583e+00, -2.3600e-02],\n",
      "        [-4.1644e+00, -1.5661e-02]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[-2.0433e+00, -1.3880e-01],\n",
      "        [-3.9409e+00, -1.9622e-02],\n",
      "        [-6.0931e+00, -2.2611e-03],\n",
      "        ...,\n",
      "        [-2.7418e-06, -1.2812e+01],\n",
      "        [-4.6492e-06, -1.2275e+01],\n",
      "        [-7.2717e-06, -1.1832e+01]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[-1.4305e-06, -1.3457e+01],\n",
      "        [-1.5497e-06, -1.3412e+01],\n",
      "        [-1.6689e-06, -1.3268e+01],\n",
      "        ...,\n",
      "        [-3.2186e-06, -1.2655e+01],\n",
      "        [-2.7418e-06, -1.2812e+01],\n",
      "        [-1.9073e-06, -1.3180e+01]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[-1.5020e-05, -1.1104e+01],\n",
      "        [-1.9193e-05, -1.0862e+01],\n",
      "        [-3.4928e-05, -1.0263e+01],\n",
      "        ...,\n",
      "        [-2.3842e-07, -1.5340e+01],\n",
      "        [-2.3842e-07, -1.5205e+01],\n",
      "        [-2.3842e-07, -1.5125e+01]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[-3.9339e-06, -1.2442e+01],\n",
      "        [-3.9339e-06, -1.2448e+01],\n",
      "        [-5.6028e-06, -1.2098e+01],\n",
      "        ...,\n",
      "        [ 0.0000e+00, -1.8710e+01],\n",
      "        [-2.3842e-07, -1.5104e+01],\n",
      "        [-2.1338e-05, -1.0754e+01]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[-3.5763e-07, -1.4991e+01],\n",
      "        [-1.1921e-07, -1.6534e+01],\n",
      "        [ 0.0000e+00, -1.7226e+01],\n",
      "        ...,\n",
      "        [ 0.0000e+00, -1.7048e+01],\n",
      "        [-1.1921e-07, -1.6187e+01],\n",
      "        [-2.3842e-07, -1.5253e+01]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[  0.0000, -20.4769],\n",
      "        [  0.0000, -20.2282],\n",
      "        [  0.0000, -17.7678],\n",
      "        ...,\n",
      "        [  0.0000, -22.5328],\n",
      "        [  0.0000, -19.5891],\n",
      "        [  0.0000, -19.3360]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[  0.0000, -23.3804],\n",
      "        [  0.0000, -22.9997],\n",
      "        [  0.0000, -20.1511],\n",
      "        ...,\n",
      "        [ -0.0520,  -2.9822],\n",
      "        [ -0.2344,  -1.5656],\n",
      "        [ -0.2248,  -1.6027]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[-5.8317e-01, -8.1674e-01],\n",
      "        [-5.2313e+00, -5.3610e-03],\n",
      "        [-6.8958e+00, -1.0125e-03],\n",
      "        ...,\n",
      "        [-8.4076e+00, -2.2325e-04],\n",
      "        [-9.0411e+00, -1.1849e-04],\n",
      "        [-1.0064e+01, -4.2557e-05]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[-8.5492e+00, -1.9370e-04],\n",
      "        [-8.5113e+00, -2.0121e-04],\n",
      "        [-9.7231e+00, -5.9841e-05],\n",
      "        ...,\n",
      "        [-1.1434e-01, -2.2252e+00],\n",
      "        [-1.7225e-01, -1.8437e+00],\n",
      "        [-7.3831e-02, -2.6427e+00]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[-1.1503e-02, -4.4709e+00],\n",
      "        [-5.9044e-03, -5.1350e+00],\n",
      "        [-4.8081e-03, -5.3399e+00],\n",
      "        ...,\n",
      "        [-1.8392e-04, -8.6010e+00],\n",
      "        [-1.9989e-04, -8.5180e+00],\n",
      "        [-2.1098e-04, -8.4638e+00]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[-1.8797e-02, -3.9834e+00],\n",
      "        [-8.5648e-03, -4.7644e+00],\n",
      "        [-9.7123e-03, -4.6392e+00],\n",
      "        ...,\n",
      "        [-9.5367e-06, -1.1564e+01],\n",
      "        [-1.9550e-05, -1.0845e+01],\n",
      "        [-2.8133e-05, -1.0477e+01]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[-1.1921e-07, -1.6193e+01],\n",
      "        [-1.1921e-07, -1.5868e+01],\n",
      "        [-8.7022e-06, -1.1654e+01],\n",
      "        ...,\n",
      "        [-1.2480e-04, -8.9891e+00],\n",
      "        [-5.9722e-05, -9.7250e+00],\n",
      "        [-2.6822e-05, -1.0526e+01]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[-1.0729e-06, -1.3721e+01],\n",
      "        [-1.1921e-07, -1.5761e+01],\n",
      "        [ 0.0000e+00, -1.6846e+01],\n",
      "        ...,\n",
      "        [ 0.0000e+00, -1.7593e+01],\n",
      "        [-2.3842e-07, -1.5315e+01],\n",
      "        [-1.1921e-07, -1.5935e+01]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[  0.0000, -17.8560],\n",
      "        [  0.0000, -20.1127],\n",
      "        [  0.0000, -23.0526],\n",
      "        ...,\n",
      "        [  0.0000, -21.3925],\n",
      "        [  0.0000, -20.1493],\n",
      "        [  0.0000, -20.0255]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[ 0.0000e+00, -2.1290e+01],\n",
      "        [ 0.0000e+00, -2.0217e+01],\n",
      "        [ 0.0000e+00, -1.9637e+01],\n",
      "        ...,\n",
      "        [-5.7637e-03, -5.1591e+00],\n",
      "        [-7.4148e-03, -4.9080e+00],\n",
      "        [-1.3749e-02, -4.2936e+00]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[-0.0065, -5.0427],\n",
      "        [-0.0063, -5.0696],\n",
      "        [-0.0064, -5.0588],\n",
      "        ...,\n",
      "        [-0.1781, -1.8129],\n",
      "        [-0.3301, -1.2689],\n",
      "        [-0.5979, -0.7985]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[-1.1204e+00, -3.9475e-01],\n",
      "        [-1.1282e+00, -3.9101e-01],\n",
      "        [-1.5110e+00, -2.4934e-01],\n",
      "        ...,\n",
      "        [-1.0156e-04, -9.1943e+00],\n",
      "        [-4.8755e-05, -9.9293e+00],\n",
      "        [-5.0782e-05, -9.8878e+00]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[-1.3757e-03, -6.5895e+00],\n",
      "        [-2.1027e-03, -6.1656e+00],\n",
      "        [-2.2129e-03, -6.1146e+00],\n",
      "        ...,\n",
      "        [-4.5349e-04, -7.6987e+00],\n",
      "        [-9.0117e-04, -7.0123e+00],\n",
      "        [-1.4850e-03, -6.5131e+00]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[-5.7943e-04, -7.4537e+00],\n",
      "        [-3.2694e-04, -8.0258e+00],\n",
      "        [-2.7069e-04, -8.2149e+00],\n",
      "        ...,\n",
      "        [-1.4305e-06, -1.3417e+01],\n",
      "        [-1.3113e-06, -1.3561e+01],\n",
      "        [-1.5974e-05, -1.1045e+01]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[-4.2676e-05, -1.0062e+01],\n",
      "        [-3.7431e-05, -1.0192e+01],\n",
      "        [-2.8371e-05, -1.0472e+01],\n",
      "        ...,\n",
      "        [ 0.0000e+00, -1.6883e+01],\n",
      "        [ 0.0000e+00, -1.7011e+01],\n",
      "        [ 0.0000e+00, -1.7541e+01]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[  0.0000, -19.5371],\n",
      "        [  0.0000, -20.7189],\n",
      "        [  0.0000, -22.5318],\n",
      "        ...,\n",
      "        [  0.0000, -20.8452],\n",
      "        [  0.0000, -20.4780],\n",
      "        [  0.0000, -21.5259]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[ 0.0000e+00, -1.9871e+01],\n",
      "        [ 0.0000e+00, -1.9401e+01],\n",
      "        [ 0.0000e+00, -1.9194e+01],\n",
      "        ...,\n",
      "        [-4.7684e-06, -1.2247e+01],\n",
      "        [-3.4571e-06, -1.2566e+01],\n",
      "        [-3.7818e-04, -7.8804e+00]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[-1.6608e-02, -4.1062e+00],\n",
      "        [-5.9283e-03, -5.1310e+00],\n",
      "        [-7.5574e-03, -4.8890e+00],\n",
      "        ...,\n",
      "        [-9.3881e-04, -6.9714e+00],\n",
      "        [-9.3512e-04, -6.9752e+00],\n",
      "        [-8.1887e-04, -7.1080e+00]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[-4.0292e-05, -1.0120e+01],\n",
      "        [-3.6477e-05, -1.0219e+01],\n",
      "        [-3.0279e-05, -1.0403e+01],\n",
      "        ...,\n",
      "        [-1.7524e-05, -1.0953e+01],\n",
      "        [-1.0848e-05, -1.1435e+01],\n",
      "        [-1.0610e-05, -1.1458e+01]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[-7.6294e-06, -1.1777e+01],\n",
      "        [-2.7656e-05, -1.0495e+01],\n",
      "        [-3.2186e-05, -1.0343e+01],\n",
      "        ...,\n",
      "        [-2.0266e-06, -1.3099e+01],\n",
      "        [-4.7684e-07, -1.4671e+01],\n",
      "        [-2.3842e-07, -1.5314e+01]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[-1.1921e-07, -1.5849e+01],\n",
      "        [-3.5763e-07, -1.5019e+01],\n",
      "        [-4.7684e-07, -1.4525e+01],\n",
      "        ...,\n",
      "        [-5.8412e-06, -1.2059e+01],\n",
      "        [-7.7486e-06, -1.1770e+01],\n",
      "        [-1.3590e-05, -1.1203e+01]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[-9.1791e-06, -1.1599e+01],\n",
      "        [-1.1086e-05, -1.1405e+01],\n",
      "        [-1.3590e-05, -1.1210e+01],\n",
      "        ...,\n",
      "        [-1.1921e-07, -1.6078e+01],\n",
      "        [-1.1921e-07, -1.6188e+01],\n",
      "        [-1.1921e-07, -1.6152e+01]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[ 0.0000e+00, -1.8523e+01],\n",
      "        [-2.3842e-07, -1.5256e+01],\n",
      "        [-1.3232e-05, -1.1236e+01],\n",
      "        ...,\n",
      "        [ 0.0000e+00, -1.7649e+01],\n",
      "        [ 0.0000e+00, -1.7970e+01],\n",
      "        [ 0.0000e+00, -1.6729e+01]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[  0.0000, -16.8897],\n",
      "        [  0.0000, -18.8161],\n",
      "        [  0.0000, -18.1737],\n",
      "        ...,\n",
      "        [  0.0000, -27.2119],\n",
      "        [  0.0000, -30.7816],\n",
      "        [  0.0000, -29.6353]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[ 0.0000e+00, -1.6772e+01],\n",
      "        [ 0.0000e+00, -1.6712e+01],\n",
      "        [ 0.0000e+00, -1.8961e+01],\n",
      "        ...,\n",
      "        [-5.9960e-05, -9.7227e+00],\n",
      "        [-6.4252e-05, -9.6521e+00],\n",
      "        [-6.6874e-05, -9.6124e+00]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[-1.8716e-05, -1.0888e+01],\n",
      "        [-1.9193e-05, -1.0859e+01],\n",
      "        [-2.5391e-05, -1.0582e+01],\n",
      "        ...,\n",
      "        [-1.8678e-04, -8.5855e+00],\n",
      "        [-5.5073e-05, -9.8069e+00],\n",
      "        [-2.1934e-05, -1.0725e+01]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[-2.0145e-03, -6.2084e+00],\n",
      "        [-1.5534e-03, -6.4681e+00],\n",
      "        [-1.4877e-03, -6.5113e+00],\n",
      "        ...,\n",
      "        [-2.9206e-05, -1.0442e+01],\n",
      "        [-2.0385e-05, -1.0799e+01],\n",
      "        [-1.6689e-05, -1.1002e+01]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[-2.5034e-06, -1.2912e+01],\n",
      "        [-4.9590e-05, -9.9107e+00],\n",
      "        [-2.7179e-05, -1.0514e+01],\n",
      "        ...,\n",
      "        [-1.1921e-07, -1.5694e+01],\n",
      "        [-2.3842e-07, -1.5476e+01],\n",
      "        [-2.3842e-07, -1.5047e+01]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[-4.1723e-06, -1.2401e+01],\n",
      "        [-4.1723e-06, -1.2392e+01],\n",
      "        [-4.2915e-06, -1.2346e+01],\n",
      "        ...,\n",
      "        [ 0.0000e+00, -1.9896e+01],\n",
      "        [ 0.0000e+00, -1.9519e+01],\n",
      "        [ 0.0000e+00, -2.1523e+01]], grad_fn=<LogSoftmaxBackward0>)\n",
      "Re: tensor(0.9549)  Pr: tensor(0.9973)  Fm: tensor(0.9756)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "prelabs = torch.abs(gtlabs - gtlabs)\n",
    "for j in range(value):\n",
    "    data = inputdata[j*batchsize:(j + 1)*batchsize].to(device)\n",
    "\n",
    "    output = network(data)\n",
    "    print(output)\n",
    "\n",
    "    prelabs[j*batchsize:(j + 1)*batchsize] = output.argmax(dim = 1)\n",
    "\n",
    "\n",
    "prefgim = prelabs.reshape(row, column)\n",
    "#gtim = gtimgs.type(torch.int64)[1000]\n",
    "gtim = gtimgs.type(torch.int64)[1000]\n",
    "\n",
    "TP, FP, TN, FN = evaluation_entry(prefgim*255, gtim)\n",
    "\n",
    "Re = TP/(TP + FN)\n",
    "Pr = TP/(TP + FP)\n",
    "Fm = (2 * Pr * Re)/(Pr + Re)\n",
    "\n",
    "print(\"Re:\", Re, \" Pr:\", Pr, \" Fm:\", Fm)\n",
    "\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize = (4,4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(bkim.detach().cpu().numpy()/255.0)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(prefgim.detach().cpu().numpy())\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3c90ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a705fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d266b4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
